# Improving and Evaluating Code Recommender Systems

------------------------------------------------------------------------------------------------------------------------

#### Author: Cristian Buratti
#### Advisor: Prof. Dr. Gabriele Bavota
#### Co-advisors: Dr. Luca Pascarella & Matteo Ciniselli
#### Date: June 2022

------------------------------------------------------------------------------------------------------------------------

This is a Master's Thesis in Software and Data Engineering, supervised by Prof. Dr. Gabriele Bavota, at the 
[Universit√† della Svizzera italiana](https://www.usi.ch/en/). This study extends the previous work done by Ciniselli et 
al. [An Empirical Study on the Usage of Transformer Models for Code Completion](https://github.com/mciniselli/T5_Replication_Package). 

------------------------------------------------------------------------------------------------------------------------

### Replication Package Structure

Our work is divided into four different tasks. For each task we created a folder inside this repository. The first task 
is the replication of the dataset used by Ciniselli et al. The second task is the extraction of code context from the 
mined repository. The third task corresponds to the training of different T5 models. The last task is development of a 
model evaluation framework.

All the required resources can be downloaded [here](https://zenodo.org/record/6603668).

------------------------------------------------------------------------------------------------------------------------

### Task 1 - Replication of the Dataset

The Python script used for the first task is available in the `miner` folder. For this entire section we are going to 
assume that the `miner` folder is the root of the project. 

Before you can run this step, you need to install all the dependencies required by the script.

```bash
pip install -r requirements.txt
```

After the installation, you need to download the text files containing the code samples from the original dataset. They
are located in a folder named `1_snapshot_miner` inside the resources you downloaded before. Put the `finetuning` folder
at `miner/data/finetuning` and run the following command:

```bash
python3 src/main.py
``` 

After the script has finished, you will find a folder named `archives`. This will contain a ZIP file for each repository
that has been successfully mined and checked out to the original version. 

Moreover, an `out` folder will contain text files regarding the mining process. One of them will be a list of failed 
repositories. Another will be called `all.csv` and will contain all the methods of the dataset. We renamed it to 
`main.csv` and it can be found under `2_main_dataset`. If you want to recreate exactly our dataset, use this file to
download the repositories and retrieve the original snapshot.

------------------------------------------------------------------------------------------------------------------------

### Task 2 - Extracting Code Context

The new folder `context` contains all the Python scripts used for the second task. As for before, after you moved into
the `context` folder, you need to install all the dependencies required by the scripts.

In order to run this script you need to put all the following inside a folder named `data`:
- `archives`: the folder containing the ZIP files of the mined repositories that was produced in the previous step
  (if you did not run the previous step, you can download our results as described above)
- `main.csv`: the CSV file containing the methods of the dataset. This file is also generated by the previous and our
  version is available as described above.
- tracing folders: these are 12 folders located under the `3_reconstruct_dataset` directory of the dataset. There exists
  one folder for each combination of dataset (Java, Android), masking level (block, construct, token) and purpose (test,
  train). Each folder contains three files: `mask.txt`, `masked_code.txt` and `tracing.txt`.

With all of these files, you can run the following command:

```bash
python3 src/main.py --extractor <extractor> --dataset <dataset> --folder <list of folders>
```

The `extractor` parameter changes what type of code context is added to the code. Its possible value are:
- `none`: no extra code will be added to the mined code
- `invoking_signature`: the signatures of all methods in the class will be sorted and added after the mined code
- `javadoc`: the JavaDoc of the method will be added after the mined code

Regarding the `dataset` parameter, it is used to change the type of dataset that will be created:
- `complete`: if set to this value, every entry of the dataset will be kept even if the context extraction failed (_i.e._,
   the `javadoc` extractor fails when the method is not documented)
- `javadoc`: this filter out all methods from the dataset that are not documented, ensuring for example that the 
  `javadoc` extractor will not fail

The last parameter that must be specified is the list of folders to be processed. The possible values change based on 
the `dataset` parameter. If using the `javadoc` dataset, then there is no distinction between Android and Java 
repositories, nor between train and test sets. The reason is that there are too few instances to create a valid dataset
(therefore the union of the two), but also that we need an evaluation dataset. In this scenario, the list of folders can 
be any subset of `['block', 'construct', 'token']`. When constructing the `complete` dataset instead, the list from 
which you can choose are the names of the 12 tracing folders you downloaded before. 

Upon completion, this script will generate some TSV files (the exact number depends on the parameters provided). They 
can then be used to train T5 models as described in the next task. 

If you want to look at all the datasets we created for each of our models, you can find them under `4_per_model_dataset`.
- `old_dataset`: are the exact same files retrieved from the replication package of Ciniselli and his research group
- `new_dataset`: is the reconstruction of their dataset, without any code context. It can be obtained by running this 
  script and setting `none` as the `extractor` parameter and `complete` as the `dataset` parameter. 
- `invoking`: is the dataset obtained by using the `invoking_signature` extractor. It enhances the old dataset with 
  the invoking signature of the methods.
- `jd_baseline`: is a subset of the `new_dataset`, obtained by using no extractor but considering only the 
  `javadoc dataset`
- `jd_invoking`: contains the same entries of `jd_baseline` but with invoking signature added
- `jd_javadoc`: contains the same entries of `jd_baseline` but with JavaDoc as code context

#### Extra independent scripts used in this task

A list of independent scripts used to aid the process of this task can be found under `src/independent_scripts`. They
were mostly utilities and are reported here for completion, but you are not required to use them to replicate our work.
- `concat.py`: concatenates the inputs and targets files generated by the evaluation notebook
- `merge_extra_ids.py`: reconstructs the dataset used by Mastropaolo et al.
  [Using Deep Learning to Generate Complete Log Statements](https://github.com/antonio-mastropaolo/LANCE).
- `shared_entries.py`: to be used after `merge_extra_ids.py`, it ensures that the pre-training dataset and the test/eval
  dataset are disjoint.
- `splitter.py`: splits the dataset into train (80%), test (10%) and validation (10%) sets. It is required when using 
  the main script to generate the `javadoc` dataset.

------------------------------------------------------------------------------------------------------------------------

### Task 3 - Jupyter Notebooks

This section focused on how to use Jupyter Notebooks to train and evaluate T5 models.

You can find four different notebooks under the `notebooks` folder on the GitHub repository:
- `complete_finetuning.ipynb`
- `javadoc_finetuning.ipynb`
- `evaluation.ipynb`
- `testing.ipynb`

The first two datasets are used to fine-tune the T5 model. Depending on the value passed to the `dataset` parameter of 
the previous section, you should use the corresponding notebook. For an in-depth guide on how to set up a Google Cloud 
Storage bucket please refer to the original [replication package](https://github.com/mciniselli/T5_Replication_Package) 
of Ciniselli et al.

The resources required to run the notebooks can be found in the `5_pretrained_model` folder you downloaded from Zenodo:
- `Ciniselli model`: the latest checkpoint of the T5 model used in the original paper. Use this model whenever you run 
  the `complete_finetuning` notebook 
- `Mastropaolo model`: the latest checkpoint of the T5 model used by Mastropaolo et al. Use this model whenever you run 
  the `javadoc_finetuning` notebook
- `configuration file`: a folder containing two `operative_config.gin` files, one for a model (Ciniselli's model) with
  200 thousand pretraining step and one (Mastropaolo's) for 500 thousand. Set the correct one based on the model you are
  using. 

The fine-tuned models are also available in the resources you downloaded, under the directory called `6_finetuned_model`.

------------------------------------------------------------------------------------------------------------------------

### Task 4 - Evaluation Framework

A folder called `framework` contains the entire code of the novel tool we implemented. As always, you are required to 
install the dependencies.

A help message can be printed by using the `--help` flag (or `-h`).

```
python3 src/main.py --help
```

*Note*: every command listed in this guide is meant to be launched from inside the **root** directory of the framework

#### Mining

The first functionality of the framework is the generation of a dataset composed only by methods covered by tests.
By running this function, the old dataset will be **overridden** and the new one will be generated. It is recommended
to refresh the dataset once a year to keep it up-to-date. Be careful as the old dataset will be **deleted** as soon as
the script starts.

The tool might take even weeks to create the dataset. As soon as it is done, it will generate a folder called `resources`
in the root directory of the framework. This will contain the one zip for each mined repository, plus the following
files:

- `repositories.csv`: a list of all the repositories in the dataset, even the ones the tool failed to mine, with 
                      detailed information (_e.g._, project status, release tag, time required to build, etc.).
- `expected.csv`: the code of each extracted method (_i.e._, the target methods).
- `tracing.csv`: a mapping of each method to repository it has been extracted from, alongside other information such as 
                 the file inside the project, the line number, coverage of the method, etc.

The folder created when we first run the script, and therefore the dataset we used in our thesis, is called `7_framework_miner_resources`.
Note that we only provide the three CSV files and not all the repositories. If you want to download all of 
them, you can filter the `repositories.csv` file for all the one with status `success` and check out the corresponding 
tag. Alongside the resources folder, we also have the input file we provided to the tool.

You can invoke the mining function by using the following command:

```bash
python3 src/main.py -a mine -i <path_to_input_file>
```

Note: the input file must be a csv file containing a column called `name`. Every other column is discarded. We recommend
using the [SEART](https://seart-ghs.si.usi.ch) tool to generate such a file. Remember to set the language to `Java` and 
exclude forks. Every other parameter is optional, although setting a minimum number of contributors/stars is encouraged.

#### Ad-hoc Dataset Generation

The naming of this functionality might be a bit confusing, because as we just learnt, the complete dataset was created 
during the mining phase (the entire content of the `resources` folder). This functionality does not change the dataset 
at all. It is in fact meant to be used by any user to obtain a subset of such dataset in order to test their model. 

The framework will provide a csv file which is a subset of `resources/expected.csv`. No masking is applied, meaning that
the user will have access to the target codes. In this way, they will be able to test their model as they prefer.

You can invoke the generation function by using the following command:

```bash
python3 src/main.py -a generate -o <path_to_output_folder> -m [line,token] -n <min_number> -x <max_number> -c [instruction,line] -t <coverage_threshold> -e <time_[m,h,d]>
```

Note: only the output `--output` (or `-o`) parameter is mandatory. The other parameters are optional.

- `--output` (or `-o`): the path to the **folder** where the dataset will be generated. The dataset will be located
                        inside this folder, and it will be called `generated.csv`.
- `--measure` (or `-m`): the type of measure used to filter the dataset. It can be set to `line` or `token`. By default,
                         it is not provided, meaning no filtering will be applied. It is meant to be used in combination
                         with the `--min` and `--max` parameters.
- `--min` (or `-n`): the minimum number of occurrences of the measure to be considered.
- `--max` (or `-x`): the maximum number of occurrences of the measure to be considered.
- `--coverage-type` (or `-c`): the type of coverage to be considered. It can be set to `instruction` or `line`. By 
                               default, it is not provided, meaning no filtering will be applied. It is meant to be used 
                               in combination with the `--coverage-threshold` parameter.
- `--coverage-threshold` (or `-t`): the minimum coverage percentage to be considered. Even though it is a percentage, it 
                                    should be provided as a simple number between 0 and 100.
- `--max-eval-time` (or `-e`): the maximum time allowed for the evaluation of the dataset. It should be provided as a 
                               number followed by one of the following units: `m` for minutes, `h` for hours, or `d` for 
                               days. By default, it is not provided, meaning no time limit will be applied.

Examples:

- by setting `--measure` to `line`, `--min` to `4` and `--max` to `15`, the generated subset of the dataset will contain 
  only the methods that have more than 4 lines and less than 15 lines.
- by setting `--coverage-type` to `instruction`, `--coverage-threshold` to `50`, the generated subset of the dataset 
  will contain only the methods that have an instruction coverage greater than 50%.
- by setting `--max-eval-time` to `10d`, invoking the evaluation function (see below) of the framework with this dataset
  will not take more than 10 days to complete. Note that the time limit is only an upper bound, thus the evaluation 
  might complete in less than 10 days. In fact, if all the predictions were correct, it will only take a few minutes.

The exact parameters we used in our thesis to test our best performing model were:

```bash
python3 src/main.py -a generate -o data -m line -n 4 -x 15 -c instruction -t 100 -e 10d
```

#### Evaluation

After the user has generated their dataset, they are supposed to mask the token they prefer and make their model predict
those. As soon as they have the predictions, they can use the framework once again to evaluate them. 

You can use the following command to evaluate your model's predictions:

```
python3 src/main.py -a evaluate -i <path_to_input_file> -o <path_to_output_folder> -b [1,2,3,4,avg]
```

- `--input` (or `-i`): the path to the csv file containing the prediction of the model. It must contain the following
                       four columns:
  - `id`: the id of the prediction as it was generated by the framework in the previous step (ad-hoc dataset generation)
  - `predicted_method`: the predicted method as a whole
  - `masked_code`: the code that was masked by the user
  - `predicted_code`: the code that was predicted by the model (_i.e._, how the model replaced the masked code)
- `--output` (or `-o`): the path to the folder where the evaluation results will be saved. Four files will be generated
                        inside this folder:
  - `summary.txt`: a small summary contained all the metrics computed by the framework
  - `log.csv`: a csv file containing the exact value of every metric for each prediction
  - `bleu_distribution.txt`: a text file containing all the BLEU scores
  - `levenshtein_distribution.txt`: a text file containing all the Levenshtein distances
- `--bleu` (or `-b`): this is the only optional parameter of this function. It can be set to either `1`, `2`, `3`, `4`,
                       or `avg`. If it is set to `1`, `2`, `3`, or `4`, the framework will only compute the BLEU score
                       for the corresponding n-grams. If it is set to `avg`, the framework will compute the average
                       BLEU score of all four n-grams. By default, it is set to `avg`.

The input file we provided to this function and the output folder generated by the tool are available in the last folder
(`8_framework_evaluate_resources`) you downloaded at the beginning of this tutorial.
