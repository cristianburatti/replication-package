# Improving and Evaluating Code Recommender Systems

------------------------------------------------------------------------------------------------------------------------

#### Author: Cristian Buratti
#### Advisor: Prof. Dr. Gabriele Bavota
#### Co-advisors: Dr. Luca Pascarella & Matteo Ciniselli
#### Date: June 2022

------------------------------------------------------------------------------------------------------------------------

This is a Master's Thesis in Software and Data Engineering, supervised by Prof. Dr. Gabriele Bavota, at the 
[Universit√† della Svizzera italiana](https://www.usi.ch/en/). This study extends the previous work done by Ciniselli et 
al. [An Empirical Study on the Usage of Transformer Models for Code Completion](https://github.com/mciniselli/T5_Replication_Package). 

------------------------------------------------------------------------------------------------------------------------

### Replication Package Structure

Our work is divided into four different tasks. For each task we created a folder inside this repository. The first task 
is the replication of the dataset used by Ciniselli et al. The second task is the extraction of code context from the 
mined repository. The third task corresponds to the training of different T5 models. The last task is development of a 
model evaluation framework.

------------------------------------------------------------------------------------------------------------------------

### Task 1 - Replication of the Dataset

The Python script used for the first task is available in the `miner` folder. For this entire section we are going to 
assume that the `miner` folder is the root of the project. 

Before you can run this step, you need to install all the dependencies required by the script.

```bash
pip install -r requirements.txt
```

After the installation, you need to download the text files containing the code samples from the original dataset. They
are available [here](TODO Link 1). Put the `finetuning` folder you at `miner/data/finetuning` and run the following command:

```bash
python3 src/main.py
``` 

After the script has finished, you will find a folder named `archives`. This will contain a ZIP file for each repository
that has been successfully mined and checked out to the original version. The results we obtained from this steps are 
available [here](TODO Link 2).

Moreover, an `out` folder will contain text files regarding the mining process. One of them will be a list of failed 
repositories. Another will be called `all.csv`and will contain all the methods of the dataset. We renamed it to 
`main.csv` and it can be downloaded [here](TODO Link 3).

------------------------------------------------------------------------------------------------------------------------

### Task 2 - Extracting Code Context

The new folder `context` contains all the Python scripts used for the second task. As for before, after you moved into
the `context` folder, you need to install all the dependencies required by the scripts.

In order to run this script you need to put all the following inside a folder named `data`:
- `archives`: the folder containing the ZIP files of the mined repositories that was produced in the previous step
  (if you did not run the previous step, you can download our results as described above)
- `main.csv`: the CSV file containing the methods of the dataset. This file is also generated by the previous and our
  version is available as described above.
- tracing folders: these are 12 folders that can be found [here](TODO Link 4). There exists one folder for each 
  combination of dataset (Java, Android), masking level (block, construct, token) and purpose (test, train). Each folder 
  contains three files: `mask.txt`, `masked_code.txt` and `tracing.txt`.

With all of these files, you can run the following command:

```bash
python3 src/main.py --extractor <extractor> --dataset <dataset> --folder <list of folders>
```

The `extractor` parameter changes what type of code context is added to the code. Its possible value are:
- `none`: no extra code will be added to the mined code
- `invoking_signature`: the signatures of all methods in the class will be sorted and added after the mined code
- `javadoc`: the javadoc of the method will be added after the mined code

Regarding the `dataset` parameter, it is used to change the type of dataset that will be created:
- `complete`: if set to this value, every entry of the dataset will be kept even if the context extraction failed (_i.e._,
   the `javadoc` extractor fails when the method is not documented)
- `javadoc`: this filter out all methods from the dataset that are not documented, ensuring for example that the 
  `javadoc` extractor will not fail

The last parameter that must be specified is the list of folders to be processed. The possible values change based on 
the `dataset` parameter. If using the `javadoc` dataset, then there is no distinction between Android and Java 
repositories, nor between train and test sets. The reason is that there are too few instances to create a valid dataset
(therefore the union of the two), but also that we need an evaluation dataset. In this scenario, the list of folders can 
be any subset of `['block', 'construct', 'token']`. When constructing the `complete` dataset instead, the list from 
which you can choose are the names of the 12 tracing folders you downloaded before. 

Upon completion, this script will generate some TSV files (the exact number depends on the parameters provided). They 
can then be used to train T5 models as described in the next task). 

If you want to look at all the datasets we created for each of our models, you can find them [here](TODO Link 5).
- `old_dataset`: are the exact same files retrieved from the replication package of Ciniselli and his research group
- `new_dataset`: is the reconstruction of their dataset, without any code context. It can be obtained by running this 
  script and setting `none` as the `extractor` parameter and `complete` as the `dataset` parameter. 
- `invoking`: is the dataset obtained by using the `invoking_signature` extractor. It enhances the old dataset with 
  the invoking signature of the methods.
- `jd_baseline`: is a subset of the `new_dataset`, obtained by using no extractor but considering only the 
  `javadoc dataset`
- `jd_invoking`: contains the same entries of `jd_baseline` but with invoking signature added
- `jd_javadoc`: contains the same entries of `jd_baseline` but with javadoc as code context

#### Extra independent scripts used in this task

A list of independent scripts used to aid the process of this task can be found under `src/independent_scripts`. They
were mostly utilities and are reported here for completion, but you are not required to use them to replicate our work.
- `concat.py`: concatenates the inputs and targets files generated by the evaluation notebook
- `merge_extra_ids.py`: reconstructs the original dataset used by Mastropaolo et al.
  [Using Deep Learning to Generate Complete Log Statements](https://github.com/antonio-mastropaolo/LANCE).
- `shared_entries.py`: to be used after `merge_extra_ids.py`, it ensures that the pre-training dataset and the test/eval
  dataset are disjoint.
- `splitter.py`: splits the dataset into train (80%), test (10%) and validation (10%) sets. It is required when using 
  the main script to generate the `javadoc` dataset.

------------------------------------------------------------------------------------------------------------------------

### Task 3 - Jupyter Notebooks

This section focused on how to use Jupyter Notebooks to train and evaluate T5 models.

You can find four different notebooks under the `notebooks` folder:
- `complete_finetuning.ipynb`
- `javadoc_finetuning.ipynb`
- `evaluation.ipynb`
- `testing.ipynb`

The first two datasets are used to fine-tune the T5 model. Depending on the value passed to the `dataset` parameter of 
the previous section, you should use the corresponding notebook. For an in-depth guide on how to set up a Google Cloud 
Storage bucket please refer to the original [replication package](https://github.com/mciniselli/T5_Replication_Package) 
of Ciniselli et al.

The resources required to run the notebooks can be found [here](TODO Link 6):
- `Ciniselli model`: the latest checkpoint of the T5 model used in the original paper. Use this model whenever you run 
  the `complete_finetuning` notebook 
- `Mastropaolo model`: the latest checkpoint of the T5 model used by Mastropaolo et al. Use this model whenever you run 
  the `javadoc_finetuning` notebook
- `configuration file`: a folder containing two `operative_config.gin` files, one for a model (Ciniselli's model) with
  200 thousand pretraining step and one (Mastropaolo's) for 500 thousand. Set the correct one based on the model you are
  using. 

The fine-tuned models that were generated during our experiments can be found [here](TODO Link 7).

------------------------------------------------------------------------------------------------------------------------

