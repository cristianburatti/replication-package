{"cells":[{"cell_type":"markdown","metadata":{"id":"hLK0f4OOSDOx"},"source":["### Creation of the environment"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"o4QXGIkfkHrW","executionInfo":{"status":"ok","timestamp":1650236830750,"user_tz":-120,"elapsed":5,"user":{"displayName":"Cristian Buratti","userId":"16128402122774352339"}}},"outputs":[],"source":["import os\n","os.environ['USE_AUTH_EPHEM'] = '0'"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":216057,"status":"ok","timestamp":1650237046804,"user":{"displayName":"Cristian Buratti","userId":"16128402122774352339"},"user_tz":-120},"id":"2h1MRzBLtex2","outputId":"ce9698c6-f023-4ad2-e65c-3232bf54ae1d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n","Collecting pip\n","  Downloading pip-22.0.4-py3-none-any.whl (2.1 MB)\n","\u001b[K     |████████████████████████████████| 2.1 MB 5.3 MB/s \n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 21.1.3\n","    Uninstalling pip-21.1.3:\n","      Successfully uninstalled pip-21.1.3\n","Successfully installed pip-22.0.4\n","Collecting git+https://github.com/google-research/text-to-text-transfer-transformer.git\n","  Cloning https://github.com/google-research/text-to-text-transfer-transformer.git to /tmp/pip-req-build-1j6z2e9p\n","  Running command git clone --filter=blob:none --quiet https://github.com/google-research/text-to-text-transfer-transformer.git /tmp/pip-req-build-1j6z2e9p\n","  Resolved https://github.com/google-research/text-to-text-transfer-transformer.git to commit c070da4626d936bab4039b007a5202f039d55f0a\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from t5==0.9.3) (1.0.0)\n","Requirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from t5==0.9.3) (2.9.1)\n","Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from t5==0.9.3) (0.5.3)\n","Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from t5==0.9.3) (0.5.0)\n","Collecting mesh-tensorflow[transformer]>=0.1.13\n","  Downloading mesh_tensorflow-0.1.19-py3-none-any.whl (366 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.4/366.4 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from t5==0.9.3) (3.2.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from t5==0.9.3) (1.21.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from t5==0.9.3) (1.3.5)\n","Collecting rouge-score\n","  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n","Collecting sacrebleu\n","  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.7/90.7 KB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from t5==0.9.3) (1.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from t5==0.9.3) (1.4.1)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting seqio-nightly\n","  Downloading seqio_nightly-0.0.7.dev20220417-py3-none-any.whl (294 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 KB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.14 in /usr/local/lib/python3.7/dist-packages (from t5==0.9.3) (1.15.0)\n","Collecting tensorflow-text\n","  Downloading tensorflow_text-2.8.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tfds-nightly\n","  Downloading tfds_nightly-4.5.2.dev202204170044-py3-none-any.whl (4.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from t5==0.9.3) (1.10.0+cu111)\n","Collecting transformers>=2.7.0\n","  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from mesh-tensorflow[transformer]>=0.1.13->t5==0.9.3) (0.16.0)\n","Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from mesh-tensorflow[transformer]>=0.1.13->t5==0.9.3) (4.0.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5==0.9.3) (4.64.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m895.2/895.2 KB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5==0.9.3) (4.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5==0.9.3) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5==0.9.3) (2019.12.20)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 KB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5==0.9.3) (3.6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5==0.9.3) (21.3)\n","Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel->t5==0.9.3) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->t5==0.9.3) (2.8.2)\n","Collecting portalocker\n","  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n","Collecting colorama\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->t5==0.9.3) (0.8.9)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->t5==0.9.3) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->t5==0.9.3) (1.1.0)\n","Requirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->t5==0.9.3) (2.8.0)\n","Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->t5==0.9.3) (0.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from tfds-nightly->t5==0.9.3) (4.1.1)\n","Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tfds-nightly->t5==0.9.3) (2.3)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tfds-nightly->t5==0.9.3) (1.7.0)\n","Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly->t5==0.9.3) (3.17.3)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tfds-nightly->t5==0.9.3) (1.1.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tfds-nightly->t5==0.9.3) (5.6.0)\n","Collecting toml\n","  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tfds-nightly->t5==0.9.3) (0.3.4)\n","Collecting etils[epath-no-tf]\n","  Downloading etils-0.5.0-py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.3/86.3 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=2.7.0->t5==0.9.3) (3.0.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.7.0->t5==0.9.3) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.7.0->t5==0.9.3) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.7.0->t5==0.9.3) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.7.0->t5==0.9.3) (1.24.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (57.4.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (1.14.0)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (13.0.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (3.1.0)\n","Collecting tf-estimator-nightly==2.8.0.dev2021122109\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 KB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (2.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (1.1.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (3.3.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (0.24.0)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (0.5.3)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (2.8.0)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (2.8.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (1.44.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (1.6.3)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (0.2.0)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath-no-tf]->tfds-nightly->t5==0.9.3) (3.8.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.7.0->t5==0.9.3) (7.1.2)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.9.3) (0.1.7)\n","Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.9.3) (21.4.0)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tfds-nightly->t5==0.9.3) (1.56.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (1.5.2)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (0.6.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (1.35.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (1.8.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (3.3.6)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (0.4.6)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (4.8)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (1.3.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5==0.9.3) (3.2.0)\n","Building wheels for collected packages: t5\n","  Building wheel for t5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for t5: filename=t5-0.9.3-py3-none-any.whl size=159555 sha256=f0415c3f486d610f50ab561ad13d463d1b381095ddb4fc393c20bcfa3f514b76\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-d55l2w40/wheels/bf/e8/36/0d7b7d2aa6a91236ea133d24d501fb1faf73d6bf2579f05c96\n","Successfully built t5\n","Installing collected packages: tokenizers, tf-estimator-nightly, sentencepiece, toml, sacremoses, pyyaml, portalocker, etils, colorama, sacrebleu, rouge-score, mesh-tensorflow, huggingface-hub, transformers, tfds-nightly, tensorflow-text, seqio-nightly, t5\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed colorama-0.4.4 etils-0.5.0 huggingface-hub-0.5.1 mesh-tensorflow-0.1.19 portalocker-2.4.0 pyyaml-6.0 rouge-score-0.0.4 sacrebleu-2.0.0 sacremoses-0.0.49 sentencepiece-0.1.96 seqio-nightly-0.0.7.dev20220417 t5-0.9.3 tensorflow-text-2.8.1 tf-estimator-nightly-2.8.0.dev2021122109 tfds-nightly-4.5.2.dev202204170044 tokenizers-0.12.1 toml-0.10.2 transformers-4.18.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRunning on TPU: grpc://10.24.154.106:8470\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"]}],"source":["%tensorflow_version 2.x\n","!pip3 install --upgrade pip\n","#!pip install -qU t5\n","!pip3 install git+https://github.com/google-research/text-to-text-transfer-transformer.git #extra_id_x support\n","\n","import functools\n","import os\n","import time\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","import tensorflow.compat.v1 as tf\n","import tensorflow_datasets as tfds\n","\n","import t5\n","\n","#Set the base dir(Google cloud bucket)\n","BASE_DIR = \"gs://code-generation\"\n","\n","#C.B.: SEQ_LENGTH for number of tokens\n","SEQ_LENGTH = 512\n","\n","if not BASE_DIR or BASE_DIR == \"gs://\":\n","  raise ValueError(\"You must enter a BASE_DIR.\")\n","ON_CLOUD = True\n","\n","\n","if ON_CLOUD:\n","  import tensorflow_gcs_config\n","  from google.colab import auth\n","  # Set credentials for GCS reading/writing from Colab and TPU.\n","  TPU_TOPOLOGY = \"2x2\"\n","  try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","    TPU_ADDRESS = tpu.get_master()\n","    print('Running on TPU:', TPU_ADDRESS)\n","  except ValueError:\n","    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n","  auth.authenticate_user()\n","  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n","  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n","\n","tf.disable_v2_behavior()\n","\n","# Improve logging.\n","from contextlib import contextmanager\n","import logging as py_logging\n","\n","if ON_CLOUD:\n","  tf.get_logger().propagate = False\n","  py_logging.root.setLevel('INFO')\n","\n","@contextmanager\n","def tf_verbosity_level(level):\n","  og_level = tf.logging.get_verbosity()\n","  tf.logging.set_verbosity(level)\n","  yield\n","  tf.logging.set_verbosity(og_level)"]},{"cell_type":"markdown","metadata":{"id":"QLpPOoErSHlC"},"source":["### Loading of tsv files\n","With this script you can load each tsv file for finetuning.\n","Please be sure that the path to all tsv files are correct"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"dhyukwjiYorb","executionInfo":{"status":"ok","timestamp":1650237046805,"user_tz":-120,"elapsed":10,"user":{"displayName":"Cristian Buratti","userId":"16128402122774352339"}}},"outputs":[],"source":["train_construct_length = 251271\n","test__construct_length = 37722\n","eval__construct_length = 36734\n","\n","train_block_length = 101646\n","test__block_length = 12706\n","eval__block_length = 12705\n","\n","train_token_length = 307779\n","test__token_length = 38480\n","eval__token_length = 38475"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"glLJUm1dxIiH","executionInfo":{"status":"ok","timestamp":1650237046805,"user_tz":-120,"elapsed":8,"user":{"displayName":"Cristian Buratti","userId":"16128402122774352339"}}},"outputs":[],"source":["#Validation(train and test on the same dataset)\n","\n","nq_tsv_path_construct = {\n","    \"train\":      BASE_DIR + '/T5_extension/ft_datasets/construct_train.tsv',\n","    \"validation\": BASE_DIR + '/T5_extension/ft_datasets/construct_test.tsv',\n","}\n","\n","num_nq_examples_construct = dict(train=train_construct_length, validation=test__construct_length)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"6u7JbyjV8GN3","executionInfo":{"status":"ok","timestamp":1650237046805,"user_tz":-120,"elapsed":7,"user":{"displayName":"Cristian Buratti","userId":"16128402122774352339"}}},"outputs":[],"source":["#Validation(train and test on the same dataset)\n","\n","nq_tsv_path_block = {\n","    \"train\":      BASE_DIR + '/T5_extension/ft_datasets/block_train.tsv',\n","    \"validation\": BASE_DIR + '/T5_extension/ft_datasets/block_test.tsv',\n","}\n","\n","num_nq_examples_block = dict(train=train_block_length, validation=test__block_length)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"E5lRmNWG8HuD","executionInfo":{"status":"ok","timestamp":1650237046805,"user_tz":-120,"elapsed":7,"user":{"displayName":"Cristian Buratti","userId":"16128402122774352339"}}},"outputs":[],"source":["#Validation(train and test on the same dataset)\n","\n","nq_tsv_path_token = {\n","    \"train\":      BASE_DIR + '/T5_extension/ft_datasets/token_train.tsv',\n","    \"validation\": BASE_DIR + '/T5_extension/ft_datasets/token_test.tsv',\n","}\n","\n","num_nq_examples_token = dict(train=train_token_length, validation=test__token_length)"]},{"cell_type":"markdown","metadata":{"id":"BNX2xguTSy4h"},"source":["### Preprocess of the dataset\n","In this step we preprocess the dataset.  \n","You have to change the path to vocab files (*vocab_model_path* and *vocab_path*)\n","We're going to preprocess all the tsv file so that T5 can use them for finetuning."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"PobLvzL18zzR","executionInfo":{"status":"ok","timestamp":1650237046806,"user_tz":-120,"elapsed":7,"user":{"displayName":"Cristian Buratti","userId":"16128402122774352339"}}},"outputs":[],"source":["from t5.data import postprocessors as t5_postprocessors\n","from t5.seqio import Feature,SentencePieceVocabulary\n","\n","\n","# # Set the path of sentencepiece model and vocab files\n","# # Must be the same used for the pre-trained phase\n","vocab_model_path = BASE_DIR + '/T5_extension/code.model'\n","vocab_path = BASE_DIR + '/T5_extension/code.vocab'\n","\n","\n","TaskRegistry = t5.data.TaskRegistry\n","TfdsTask = t5.data.TfdsTask\n","\n","\n","def get_default_vocabulary():\n","  return SentencePieceVocabulary(vocab_model_path, 100)\n","\n","DEFAULT_OUTPUT_FEATURES = {\n","    \"inputs\": Feature(\n","        vocabulary=get_default_vocabulary(), add_eos=True, required=False),\n","\n","    \"targets\": Feature(\n","        vocabulary=get_default_vocabulary(), add_eos=True)\n","}"]},{"cell_type":"markdown","metadata":{"id":"mn-DMH5FkSO2"},"source":["JAVA CONSTRUCT"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3445,"status":"ok","timestamp":1650237050244,"user":{"displayName":"Cristian Buratti","userId":"16128402122774352339"},"user_tz":-120},"id":"K0NTLbyXvkCs","outputId":"6d58dbe2-e4b7-49f3-e13e-9278c0cd2af6"},"outputs":[{"output_type":"stream","name":"stdout","text":["A few raw train examples...\n","{'input': b'private String formatEventDateRange(Date beginDate, Date endDate) { if ( <extra_id_0>) { if (isEndOfDay(endDate)) { return formatEventDate(beginDate); } else if (isMidnight(beginDate)) { return formatEventDate(beginDate) + \" until \" + eventOutDayOnlyDf.format(endDate); } else { return formatEventDate(beginDate) + \" - \" + eventOutTimeOnlyDf.format(endDate); } } else { return formatEventDate(beginDate) + \" - \" + formatEventDate(endDate); } } <SEP> /** Get the string representation of an event date range. */', 'output': b'DateUtils.isSameDay(beginDate, endDate)'}\n","{'input': b'private String formatEventDateRange(Date beginDate, Date endDate) { if (DateUtils.isSameDay(beginDate, endDate)) { if ( <extra_id_0>) { return formatEventDate(beginDate); } else if (isMidnight(beginDate)) { return formatEventDate(beginDate) + \" until \" + eventOutDayOnlyDf.format(endDate); } else { return formatEventDate(beginDate) + \" - \" + eventOutTimeOnlyDf.format(endDate); } } else { return formatEventDate(beginDate) + \" - \" + formatEventDate(endDate); } } <SEP> /** Get the string representation of an event date range. */', 'output': b'isEndOfDay(endDate)'}\n","{'input': b'private String formatEventDateRange(Date beginDate, Date endDate) { if (DateUtils.isSameDay(beginDate, endDate)) { if (isEndOfDay(endDate)) { return formatEventDate(beginDate); } else if ( <extra_id_0>) { return formatEventDate(beginDate) + \" until \" + eventOutDayOnlyDf.format(endDate); } else { return formatEventDate(beginDate) + \" - \" + eventOutTimeOnlyDf.format(endDate); } } else { return formatEventDate(beginDate) + \" - \" + formatEventDate(endDate); } } <SEP> /** Get the string representation of an event date range. */', 'output': b'isMidnight(beginDate)'}\n","{'input': b'private String formatEventDateRange(Date beginDate, Date endDate) { if (DateUtils.isSameDay(beginDate, endDate)) { if (isEndOfDay(endDate)) { return formatEventDate(beginDate); } else if (isMidnight(beginDate)) { return formatEventDate(beginDate) + \" until \" + eventOutDayOnlyDf.format( <extra_id_0>); } else { return formatEventDate(beginDate) + \" - \" + eventOutTimeOnlyDf.format(endDate); } } else { return formatEventDate(beginDate) + \" - \" + formatEventDate(endDate); } } <SEP> /** Get the string representation of an event date range. */', 'output': b'endDate'}\n","{'input': b'private String formatEventDateRange(Date beginDate, Date endDate) { if (DateUtils.isSameDay(beginDate, endDate)) { if (isEndOfDay(endDate)) { return formatEventDate(beginDate); } else if (isMidnight(beginDate)) { return formatEventDate(beginDate) + \" until \" + eventOutDayOnlyDf.format(endDate); } else { return formatEventDate(beginDate) + \" - \" + eventOutTimeOnlyDf.format( <extra_id_0>); } } else { return formatEventDate(beginDate) + \" - \" + formatEventDate(endDate); } } <SEP> /** Get the string representation of an event date range. */', 'output': b'endDate'}\n"]}],"source":["def nq_construct(split, shuffle_files=True):\n","  # We only have one file for each split.\n","  del shuffle_files\n","\n","   # Load lines from the text file as examples.\n","\n","  ds = tf.data.TextLineDataset(nq_tsv_path_construct[split])\n","  ds = ds.map(\n","      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n","                        field_delim=\"\\t\", use_quote_delim=False),\n","      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","  \n","  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n","  return ds\n","\n","print(\"A few raw train examples...\")\n","for ex in tfds.as_numpy(nq_construct(\"train\").take(5)):\n","  print(ex)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"4bJZPQgjxKZ1","executionInfo":{"status":"ok","timestamp":1650237050245,"user_tz":-120,"elapsed":13,"user":{"displayName":"Cristian Buratti","userId":"16128402122774352339"}}},"outputs":[],"source":["def construct_preprocessing(ds):\n","  \n","  def to_inputs_and_targets(ex):\n","\n","        inputs = tf.strings.join(['CONSTRUCT:' + ex['input']], separator=' ')\n","        class_label = tf.strings.join([ex['output']], separator=' ')\n","        return {'inputs': inputs, 'targets': class_label }\n","    \n","  return ds.map(to_inputs_and_targets, \n","                num_parallel_calls=tf.data.experimental.AUTOTUNE)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1650237050245,"user":{"displayName":"Cristian Buratti","userId":"16128402122774352339"},"user_tz":-120},"id":"h3jAg8Zhx_Ep","outputId":"db82d897-221c-4e4f-b1f0-2221eb174d31"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<t5.data.dataset_providers.FunctionTask at 0x7f58f2779b50>"]},"metadata":{},"execution_count":10}],"source":["t5.data.TaskRegistry.remove('construct')\n","t5.data.TaskRegistry.add(\n","    \"construct\",\n","    dataset_fn=nq_construct,\n","    splits=[\"train\", \"validation\"],\n","    text_preprocessor=[construct_preprocessing],\n","    output_features = DEFAULT_OUTPUT_FEATURES,\n","    metric_fns=[t5.evaluation.metrics.accuracy],\n","    num_input_examples=num_nq_examples_construct\n",")"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3274,"status":"ok","timestamp":1650237053831,"user":{"displayName":"Cristian Buratti","userId":"16128402122774352339"},"user_tz":-120},"id":"e71p9JIFyYHm","outputId":"e3db2cb8-bfc5-46f7-f98a-fc4aba097965"},"outputs":[{"output_type":"stream","name":"stdout","text":["A few preprocessed training examples...\n","{'inputs_pretokenized': b'CONSTRUCT:public void onBindViewHolder(@NonNull Rule holder, int position) { holder.setLast(hasScroll && position == lastItem); holder.setValue(values.get( <extra_id_0>)); } <SEP> /** Put data into the rule at the given position. */', 'inputs': array([12094, 30813,    78,  2825,    65,    44,  9748,  1351,  2984,\n","       23610, 18588,  7096,  6221,     9,    99,   961,    91,    20,\n","        6221,    12,  2060,  2844,   451,  6120, 10795,   311,   961,\n","         126,   315,   847,  4767,  6221,    12,  2060,   275,   451,\n","       14655,    12,  1323,   451, 32099,    91,  4767,    21,    52,\n","       25390,  1302,     6, 23808,  5583,   261,   216,    11,  1368,\n","          68,    11,  1017,   961,    12,   651,    98,     1],\n","      dtype=int32), 'targets_pretokenized': b'position', 'targets': array([961,   1], dtype=int32)}\n","{'inputs_pretokenized': b'CONSTRUCT:private void prepForModal() { RelativeLayout blackOutLayer = (RelativeLayout)findViewById(R.id.settings_fog); RelativeLayout mainBackLayer = (RelativeLayout)findViewById( <extra_id_0>); ListView list = getListView(); mainBackLayer.setEnabled(false); list.setEnabled(false); clearButton.setEnabled(false); refineButton.setEnabled(false); blackOutLayer.setVisibility(View.VISIBLE); } <SEP> /** * Helper method to dim out the background and make the list view unclickable in preparation to display a modal */', 'inputs': array([12094, 30813,    78, 14566,    65, 11162,   608, 19045,  1927,\n","          20,     6, 11038,  2866,  1436,  3236,  4200,    24,     4,\n","       11038,  2866,    91, 13292,  1351,  5645,   451,   806,    12,\n","         368,    12, 16314,    35,   228,  5032,  4767,     6, 11038,\n","        2866,   796,  7303,  4200,    24,     4, 11038,  2866,    91,\n","       13292,  1351,  5645,   451, 32099,  4767,     6, 22616,   335,\n","          24,    51, 22616, 12329,   796,  7303,  4200,    12,  2060,\n","        1981,   451,  5346,  4767,   335,    12,  2060,  1981,   451,\n","        5346,  4767,   689,  2791,    12,  2060,  1981,   451,  5346,\n","        4767, 10997,  2791,    12,  2060,  1981,   451,  5346,  4767,\n","        1436,  3236,  4200,    12,  2060, 11029,   451,  1351,    12,\n","       30074,  4767,    21,    52, 25390,  1302,     6, 23808,   651,\n","           6,  1211,   765,    16,  7008,   105,    11,  2603,    13,\n","         157,    11,   335,   717,   382,  9401,   202,    22,  5402,\n","          16,  1582,    18, 18036,   651,    98,     1], dtype=int32), 'targets_pretokenized': b'R.id.object_index_main', 'targets': array([ 259,   12,  368,   12, 7427,   35, 4921,   35, 9452,    1],\n","      dtype=int32)}\n","{'inputs_pretokenized': b'CONSTRUCT:private void openArticle(int pos) { String url = mAdapter.getPosts().get( <extra_id_0>).getLinkURL(); Intent i = new Intent(Intent.ACTION_VIEW, Uri.parse(url)); startActivity(i); } <SEP> /** * Open article in either ArticleView activity or any installed web browser * @param pos position of the article in the dataset */', 'inputs': array([12094, 30813,    78, 14566,    65,   581, 11116,   451,  2886,\n","        1679,    91,    20,    53,  1423,    24,   101,  2690,    12,\n","        1323,  4278,     8,  1927,    12,  1323,   451, 32099,   437,\n","        1323,  2070,  1650, 12329, 23938,   107,    24,    39, 23938,\n","         451, 16313,    12, 11840,    35, 14891,     9, 16566,    12,\n","       15133,   451,  5418,    91,  4767,   305,  5733,   451,   264,\n","        4767,    21,    52, 25390,  1302,     6, 23808,   651,  2606,\n","        1922,    22,  1547, 11232,  1351,  2223,    66,   151,  3535,\n","        1040,  4475,   651,    55, 11404,  1679,   961,    19,    11,\n","        1922,    22,    11,  4742,   651,    98,     1], dtype=int32), 'targets_pretokenized': b'pos', 'targets': array([1679,    1], dtype=int32)}\n","{'inputs_pretokenized': b'CONSTRUCT:public SipProfile buildAccount(SipProfile account) { SipProfile acc = super.buildAccount( <extra_id_0>); acc.proxies = null; String encodedUser = SipUri.encodeUser(accountUsername.getText().trim()); account.acc_id = \"0\"+encodedUser+\" <sip:\" + encodedUser + \"@\"+getDomain()+\">\"; return acc; } <SEP> /** * {@inheritDoc} */', 'inputs': array([12094, 30813,    78,  2825,     6, 18012,  3497,   299,  3088,\n","         451, 18012,  3497,   962,    91,    20,     6, 18012,  3497,\n","        3551,    24,   436,    12, 11486,  3088,   451, 32099,  4767,\n","        3551,    12,  5302,    15,   338,    24,    70,   204,    53,\n","        6841,   891,    24,     6, 18012,  2080,    12, 17412,   891,\n","         451, 10142,  8772,    12,  1323,  1147,  1927,    12, 22873,\n","       27706,   962,    12, 16072,    35,   368,    24,  6835,  1407,\n","       17412,    34,   891,  1407,    26,    52,     8,  2373,  4033,\n","          75,  6841,   891,    75,  5253,    26,  1407,  1323,  3696,\n","        1927,  1407,    26,  1931,   204,    40,  3551,   204,    21,\n","          52, 25390,  1302,     6, 23808,   651,    20,  1460, 30895,\n","        3709,  2202,   651,    98,     1], dtype=int32), 'targets_pretokenized': b'account', 'targets': array([962,   1], dtype=int32)}\n","{'inputs_pretokenized': b'CONSTRUCT:private void printObject(Destination object, final ViewHolder holder) { holder.txtNome.setText( <extra_id_0>); holder.txtRegiao.setText(object.getRegiao()); holder.txtPais.setText(object.getPais()); Picasso.with(context).load(object.getImage_url()) .fit() .centerCrop() .placeholder(R.drawable.holder) .into(holder.imageView); } <SEP> /** * M\\xc3\\xa9todo para imprimir o objeto nos campos correspondentes * @param object * @param holder */', 'inputs': array([12094, 30813,    78, 14566,    65,   948,   587,   451,  7941,\n","         921,     9,    85,  3977,  2984,  6221,    91,    20,  6221,\n","          12,  5290,  1276,  2022,    12,  2060,  1147,   451, 32099,\n","        4767,  6221,    12,  5290, 12105,   484,   233,    12,  2060,\n","        1147,   451,  7427,    12,  1323, 12105,   484,   233, 27706,\n","        6221,    12,  5290, 10206,   660,    12,  2060,  1147,   451,\n","        7427,    12,  1323, 10206,   660, 27706, 11793, 19615,    12,\n","        3439,   451,  8556,   437,  8818,   451,  7427,    12,  1323,\n","        2256,    35,  5418, 23620,     7,  4319,  1927,     7,  9760,\n","       27295,  1927,     7,  2954, 10726,   451,   806,    12, 14470,\n","         202,    12, 10726,    91,     7, 22645,   451, 10726,    12,\n","        7767,  1351,  4767,    21,    52, 25390,  1302,     6, 23808,\n","         651,   195, 18697,    46,  7409,  4313,  2730,  8180,   835,\n","         399,   845,    63,   528,   210,     8,  1929,  1628, 10663,\n","        1000,   122,   651,    55, 11404,   921,   651,    55, 11404,\n","        6221,   651,    98,     1], dtype=int32), 'targets_pretokenized': b'object.getNome()', 'targets': array([ 921,   12, 1323, 1276, 2022, 1927,    1], dtype=int32)}\n"]}],"source":["nq_task = t5.data.TaskRegistry.get(\"construct\")\n","ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": SEQ_LENGTH, \"targets\": SEQ_LENGTH})\n","print(\"A few preprocessed training examples...\")\n","for ex in tfds.as_numpy(ds.take(5)):\n","  print(ex)\n"]},{"cell_type":"markdown","metadata":{"id":"yB-KY403kcCn"},"source":["JAVA TOKEN"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2049,"status":"ok","timestamp":1650237055876,"user":{"displayName":"Cristian Buratti","userId":"16128402122774352339"},"user_tz":-120},"id":"UNi7HPiOz27q","outputId":"8635e91d-2827-4402-941f-2700de23f848"},"outputs":[{"output_type":"stream","name":"stdout","text":["A few raw valid examples...\n","{'input': b'public static Event getInfo <extra_id_0> Result result = Caller.getInstance().call(\"event.getInfo\", apiKey, \"event\", eventId); return ResponseBuilder.buildItem(result, Event.class); } <SEP> /** * Get the metadata for an event on Last.fm. Includes attendance and lineup information. * * @param eventId The numeric last.fm event id * @param apiKey A Last.fm API key. * @return Event metadata */', 'output': b'(String eventId, String apiKey) {'}\n","{'input': b'private <extra_id_0> if (mGoogleApiClient.isConnected()) { Plus.AccountApi.clearDefaultAccount(mGoogleApiClient); Plus.AccountApi.revokeAccessAndDisconnect(mGoogleApiClient) .setResultCallback(new ResultCallback<Status>() { @Override public void onResult(Status arg0) { Log.e(TAG, \"User access revoked!\"); mGoogleApiClient.connect(); updateUI(false); } }); } } <SEP> /** * Revoking access from google * */', 'output': b'void revokeGplusAccess() {'}\n","{'input': b'private void revokeGplusAccess() { if (mGoogleApiClient.isConnected <extra_id_0> Plus.AccountApi.clearDefaultAccount(mGoogleApiClient); Plus.AccountApi.revokeAccessAndDisconnect(mGoogleApiClient) .setResultCallback(new ResultCallback<Status>() { @Override public void onResult(Status arg0) { Log.e(TAG, \"User access revoked!\"); mGoogleApiClient.connect(); updateUI(false); } }); } } <SEP> /** * Revoking access from google * */', 'output': b'()) {'}\n","{'input': b'private void revokeGplusAccess() { if (mGoogleApiClient.isConnected()) { Plus.AccountApi.clearDefaultAccount <extra_id_0> Plus.AccountApi.revokeAccessAndDisconnect(mGoogleApiClient) .setResultCallback(new ResultCallback<Status>() { @Override public void onResult(Status arg0) { Log.e(TAG, \"User access revoked!\"); mGoogleApiClient.connect(); updateUI(false); } }); } } <SEP> /** * Revoking access from google * */', 'output': b'(mGoogleApiClient);'}\n","{'input': b'private void revokeGplusAccess() { if (mGoogleApiClient.isConnected()) { Plus.AccountApi.clearDefaultAccount(mGoogleApiClient); Plus.AccountApi.revokeAccessAndDisconnect(mGoogleApiClient <extra_id_0> .setResultCallback(new ResultCallback<Status>() { @Override public void onResult(Status arg0) { Log.e(TAG, \"User access revoked!\"); mGoogleApiClient.connect(); updateUI(false); } }); } } <SEP> /** * Revoking access from google * */', 'output': b')'}\n"]}],"source":["def nq_token(split, shuffle_files=False):\n","  # We only have one file for each split.\n","  del shuffle_files\n","\n","  # Load lines from the text file as examples.\n","  ds = tf.data.TextLineDataset(nq_tsv_path_token[split])\n","  ds = ds.map(\n","      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n","                        field_delim=\"\\t\", use_quote_delim=False),\n","      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","  \n","  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n","  return ds\n","\n","print(\"A few raw valid examples...\")\n","for ex in tfds.as_numpy(nq_token(\"validation\").take(5)):\n","  print(ex)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"VvDAbgNY0B4Y","executionInfo":{"status":"ok","timestamp":1650237055876,"user_tz":-120,"elapsed":14,"user":{"displayName":"Cristian Buratti","userId":"16128402122774352339"}}},"outputs":[],"source":["def token_preprocessing(ds):\n","  \n","  def to_inputs_and_targets(ex):\n","\n","        inputs = tf.strings.join(['TOKEN:' + ex['input']], separator=' ')\n","        class_label = tf.strings.join([ex['output']], separator=' ')\n","        return {'inputs': inputs, 'targets': class_label }\n","    \n","  return ds.map(to_inputs_and_targets, \n","                num_parallel_calls=tf.data.experimental.AUTOTUNE)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1650237055877,"user":{"displayName":"Cristian Buratti","userId":"16128402122774352339"},"user_tz":-120},"id":"-Mm6AQfw0INC","outputId":"38e953ad-b86f-4ef1-da00-9bfc37d5a9ff"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<t5.data.dataset_providers.FunctionTask at 0x7f59086d0390>"]},"metadata":{},"execution_count":14}],"source":["t5.data.TaskRegistry.remove('token')\n","t5.data.TaskRegistry.add(\n","    \"token\",\n","    dataset_fn=nq_token,\n","    splits=[\"train\", \"validation\"],\n","    text_preprocessor=[token_preprocessing],\n","    output_features = DEFAULT_OUTPUT_FEATURES,\n","    metric_fns=[t5.evaluation.metrics.accuracy],\n","    num_input_examples=num_nq_examples_token\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2248,"status":"ok","timestamp":1650237058118,"user":{"displayName":"Cristian Buratti","userId":"16128402122774352339"},"user_tz":-120},"id":"qnf25qt10Wkl","outputId":"0ae65fac-d34e-4db6-bd7c-735881e73709"},"outputs":[{"output_type":"stream","name":"stdout","text":["A few preprocessed training examples...\n","{'inputs_pretokenized': b'TOKEN:public BlockDeviceObserver(Shell rootShell, PartitionListener listener) { super(\"/dev/block/\", FileObserver.CREATE | FileObserver.DELETE); mVolumes = new HashMap<String, Volume>(); mListener = listener; mHandler = new Handler(Looper.getMainLooper <extra_id_0> mRootShell = rootShell; detectDevices(); } <SEP> /** * Creates a new block device observer. * Does not start observing until {@link #startWatching()} is called. * @param rootShell The shell to execute mount commands in. * @param listener A listener to receive block device events. * Calls are received on the main thread. */', 'inputs': array([18964,    78,  2825,  3901,  3790,  8655,   451, 13259,  1475,\n","       13259,     9,  7277,  1128,  2173,    91,    20,   436,   451,\n","          26,    98, 11361,    98,  9062,    98,    26,     9,   629,\n","        8655,    12,  8543,  1338,   629,  8655,    12, 13112,  4767,\n","         101,  7491,     8,    24,    39,  1410,  4298,   529,     9,\n","       10053,  1302, 12329,   101,  1128,    24,  2173,   204,   101,\n","         792,    24,    39, 14748,   451,  9743,    83,    12,  1323,\n","        7394,  9743,    83, 32099,   101,  2908, 13259,    24,  1475,\n","       13259,   204,  6037,  3790,     8, 12329,    21,    52, 25390,\n","        1302,     6, 23808,   651,  3877,     8,    18,    39,  1604,\n","        1479, 11041,    12,   651,     6,  7399,    86,   305, 16110,\n","          27,   807,    20,  1460,  5251,  1664,  3890,  9995,    27,\n","        1927,  2202,    28,  1135,    12,   651,    55, 11404,  1475,\n","       13259,    69,  5966,    16,   910,  2305, 10200,    22,    12,\n","         651,    55, 11404,  2173,   100,  2173,    16,  1118,  1604,\n","        1479,  1160,    12,   651,  2745,     8,    58,  1561,    44,\n","          11,   796,  3292,    12,   651,    98,     1], dtype=int32), 'targets_pretokenized': b'());', 'targets': array([    6, 27706,     1], dtype=int32)}\n","{'inputs_pretokenized': b'TOKEN:public MosaicLayout(Context context) { super(context); this.mContext = context; if (frameLayout == null) frameLayout = new FrameLayout(mContext); Log.d(\"MosaicLayout\", \"MosaicLayout(Context context)\" <extra_id_0> } <SEP> /** * @param context: the context where the layout works in * * @description Constructor that pass an object of the context */', 'inputs': array([18964,    78,  2825,     6, 23804,  2866,   451,   282,   330,\n","          91,    20,   436,   451,  8556,  4767,    49,    12,   104,\n","         282,    24,   330,   204,    56,     4,  9119,  2866,   126,\n","          70,    91,  2275,  2866,    24,    39,  9049,  2866,   451,\n","         104,   282,  4767,  4763,    12,    34,   451,    26, 23804,\n","        2866,    26,     9,    29, 23804,  2866,   451,   282,   330,\n","          91,    26, 32099,    21,    52, 25390,  1302,     6, 23808,\n","         651,    55, 11404,   330,    78,    11,   330,   298,    11,\n","        3048,  1434,    22,   651,   651,    55,  8430, 16075,    43,\n","        1389,    81,   921,    19,    11,   330,   651,    98,     1],\n","      dtype=int32), 'targets_pretokenized': b');', 'targets': array([  5, 204,   1], dtype=int32)}\n","{'inputs_pretokenized': b'TOKEN:public static void invokeNow(final Runnable doRun) { if (SwingUtilities.isEventDispatchThread()) { doRun.run(); } else { try { SwingUtilities.invokeAndWait(doRun); } catch (InvocationTargetException e) { throw new RuntimeException( \"Unexpected error during Runnable execution\" <extra_id_0> e.getCause()); } catch (InterruptedException e) { Gdx.app.debug(SwingEDTUtils.class.getName(), \"Swing thread interrupted\", e); } } } <SEP> /** * Execute {@code doRun} in the EDT now. The calling thread waits for the * completion of {@code doRun}. * * * This method use internally * {@link javax.swing.SwingUtilities#invokeAndWait(Runnable)}, any exception * thrown during the execution of {@code doRun} is caught in the calling * thread and wrapped as a {@code RuntimeException}. * * @param doRun * {@code Runnable} to be run. * * @throws RuntimeException * if {@code doRun} throws an exception during its execution. */', 'inputs': array([18964,    78,  2825,   190,    65,  2461,  1836,   451,  8685,\n","        6854,   123,  3670,    91,    20,    56,     4,   176, 10164,\n","        8440,    12,   660,   521, 11561,  3369, 23620,    20,   123,\n","        3670,    12,  5842, 12329,    21,   203,    20,   226,    20,\n","       29141,    12, 24903, 23198,   451,  1853,  3670,  4767,    21,\n","         328,     4, 14636,   117,    77,    91,    20,   235,    39,\n","           6,  1894,   451,    29,  7835,  1020,   589,  6854,  3668,\n","          26, 32099,    77,    12,  1323,  8578, 27706,    21,   328,\n","           4, 19992,   117,    77,    91,    20,   346,    34,    15,\n","          12,  3828,    12, 19548,   451,   176, 10164,  1854,   507,\n","         821,    12,  2570,    12,  1323,   186, 15496,    29,   176,\n","       10164,  3292, 17738,    26,     9,    77,  4767,    21,    21,\n","          21,    52, 25390,  1302,     6, 23808,   651, 17064,    20,\n","        1460,  3927,   123,  3670,  2202,    22,    11,   266, 10160,\n","         295,    12,    69,  4381,  3292,  1916,     8,    32,    11,\n","         651,  5768,    19,    20,  1460,  3927,   123,  3670,  2202,\n","          12,   651,   651,   651,   214,   765,   197,  2242,    59,\n","         651,    20,  1460,  5251,  8735,    12,     8, 10164,    12,\n","         176, 10164,  8440,  2716, 24903, 23198,   451, 10893, 23836,\n","           9,   151,  1321,   651,  6321,   589,    11,  3668,    19,\n","          20,  1460,  3927,   123,  3670,  2202,    28,  5028,    22,\n","          11,  4381,   651,  3292,    13,  6517,    64,    18,    20,\n","        1460,  3927,     6,  1894,  2202,    12,   651,   651,    55,\n","       11404,   123,  3670,   651,    20,  1460,  3927,  6854,  2202,\n","          16,    54,   506,    12,   651,   651,    55, 13150,     8,\n","           6,  1894,   651,    56,    20,  1460,  3927,   123,  3670,\n","        2202,   108,    81,  1321,   589,   242,  3668,    12,   651,\n","          98,     1], dtype=int32), 'targets_pretokenized': b',', 'targets': array([17,  1], dtype=int32)}\n","{'inputs_pretokenized': b'TOKEN:public void topicClick(final String id) { forum.runOnUiThread(new Runnable() { @Override public void run() { try { forum.getRecords(ForumJob.TOPIC, Integer.parseInt(id), \"1\"); } catch (Exception <extra_id_0> Theme.Snackbar(forum, R.string.toast_error_Records); } } }); } <SEP> /** * Get the posts from a certain topic. */', 'inputs': array([18964,    78,  2825,    65,  2316,  8259,   451,  8685,    53,\n","         491,    91,    20,  5458,    12,  5842,  1033, 10859,  3369,\n","         451,  1973,  6854,  1927,    20,    55,  8203,    38,    65,\n","         506,  1927,    20,   226,    20,  5458,    12,  1323,  4856,\n","         451,  8640,  1827,    12, 18062,     9,   457,    12, 15133,\n","        2592,   451,   368,   425,  3813,  4767,    21,   328,     4,\n","         117, 32099, 12964,    12,   176,  6332,  1621,   451, 26089,\n","           9,   259,    12,  4635,    12,    46,  2796,    35,  3489,\n","          35,  4856,  4767,    21,    21,    21,  4767,    21,    52,\n","       25390,  1302,     6, 23808,   651,  1663,    11,  4593,    67,\n","          18,     6,  1154,  2316,    12,   651,    98,     1],\n","      dtype=int32), 'targets_pretokenized': b'e) {', 'targets': array([77, 91, 20,  1], dtype=int32)}\n","{'inputs_pretokenized': b'TOKEN:private Bitmap crop(BitmapFactory.Options outOptions) throws IOException { InputStream inputStream = openBitmapInputStream(); try { Bitmap fullResolutionBitmap = BitmapFactory.decodeStream(inputStream, null, outOptions); if (fullResolutionBitmap <extra_id_0> throw new IOException(\"Cannot decode bitmap: \" + mUri); } return Bitmap.createBitmap(fullResolutionBitmap, mX, mY, mWidth, mHeight); } finally { if (inputStream != null) { inputStream.close(); } } } <SEP> /** * Reads and crops the bitmap. * @param outOptions Bitmap options, useful to determine {@code outMimeType}. */', 'inputs': array([18964,    78, 14566,     6, 17469,  7807,   451, 17469,   438,\n","          12,  1044,   105,  1044,    91,   108,   428,    20,  2882,\n","        7311,    24,   581, 17469,  2458, 12329,   226,    20,     6,\n","       17469,   543,  9362, 17469,    24,     6, 17469,   438,    12,\n","       19491,  1235,   451,  6261,  1235,     9,    70,     9,   105,\n","        1044,  4767,    56,     4, 11282,  9362, 17469, 32099,   235,\n","          39,   428,   451,    26,  5825,  4239, 24327,    78,    29,\n","          75,   101,  2080,  4767,    21,    40,     6, 17469,    12,\n","        5004, 17469,   451, 11282,  9362, 17469,     9,   101,   723,\n","           9,   101,   985,     9,   101,  4238,     9,   101,  4495,\n","        4767,    21,   966,    20,    56,     4,  6261,  1235,   161,\n","          70,    91,    20,  7311,    12, 11161, 12329,    21,    21,\n","          21,    52, 25390,  1302,     6, 23808,   651,  4203,     8,\n","          13,  7807,     8,    11, 24327,    12,   651,    55, 11404,\n","         105,  1044,     6, 17469,   685,     9,   197,   245,    16,\n","        3006,    20,  1460,  3927,   105, 14086,  2202,    12,   651,\n","          98,     1], dtype=int32), 'targets_pretokenized': b'== null) {', 'targets': array([126,  70,  91,  20,   1], dtype=int32)}\n"]}],"source":["nq_task = t5.data.TaskRegistry.get(\"token\")\n","ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": SEQ_LENGTH, \"targets\": SEQ_LENGTH})\n","print(\"A few preprocessed training examples...\")\n","for ex in tfds.as_numpy(ds.take(5)):\n","  print(ex)\n"]},{"cell_type":"markdown","metadata":{"id":"ZIe-u5l9ke6x"},"source":["JAVA BLOCK"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1961,"status":"ok","timestamp":1650237060076,"user":{"displayName":"Cristian Buratti","userId":"16128402122774352339"},"user_tz":-120},"id":"yr0TT18ejMtY","outputId":"7a6f47d8-8939-4388-8ea5-8d02f28e4e7a"},"outputs":[{"output_type":"stream","name":"stdout","text":["A few raw valid examples...\n","{'input': b'public void removeItem(int position, boolean argIsSilent) { if (position < 0) <extra_id_0> int size = mInternalPlaylist.size(); if (size > position) { mInternalPlaylist.remove(position); } if (!argIsSilent) notifyPlaylistChanged(); } <SEP> /** * * @param position */', 'output': b'{ VendorCrashReporter.report(\"Playlist remove\", \"Position must be greater or equal to zero\"); return; }'}\n","{'input': b'public void removeItem(int position, boolean argIsSilent) { if (position < 0) { VendorCrashReporter.report(\"Playlist remove\", \"Position must be greater or equal to zero\"); return; } int size = mInternalPlaylist.size(); if (size > position) <extra_id_0> if (!argIsSilent) notifyPlaylistChanged(); } <SEP> /** * * @param position */', 'output': b'{ mInternalPlaylist.remove(position); }'}\n","{'input': b'public void removeItem(int position, boolean argIsSilent) { if (position < 0) { VendorCrashReporter.report(\"Playlist remove\", \"Position must be greater or equal to zero\"); return; } int size = mInternalPlaylist.size(); if (size > position) { mInternalPlaylist.remove(position); } if (!argIsSilent) <extra_id_0> } <SEP> /** * * @param position */', 'output': b'notifyPlaylistChanged();'}\n","{'input': b'public static void onMainActivityHidden(int depth) { currentBridge = genericBridge; if (guiListener != null) <extra_id_0> AndroidSingleton.activity = null; } <SEP> /** * Call this when the main activity is being destroyed. If the main activity * is not re-created soon, the service will be stopped, if there are no * running jobs left. */', 'output': b'{ guiListener.guiHidden(depth); }'}\n","{'input': b'public void autoCorrectCapitalization() { String enteredText = getText().toString(); if (enteredText != null && !choices.contains(enteredText) && choicesAllLowerCase.contains(enteredText.toLowerCase())) <extra_id_0> } <SEP> /** * If the user has entered a valid answer but with different case, change the case for them */', 'output': b'{ int index = choicesAllLowerCase.indexOf(enteredText.toLowerCase()); setText(choices.get(index)); }'}\n"]}],"source":["def nq_block(split, shuffle_files=False):\n","  # We only have one file for each split.\n","  del shuffle_files\n","\n","  # Load lines from the text file as examples.\n","  ds = tf.data.TextLineDataset(nq_tsv_path_block[split])\n","  ds = ds.map(\n","      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n","                        field_delim=\"\\t\", use_quote_delim=False),\n","      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","  \n","  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n","  return ds\n","\n","print(\"A few raw valid examples...\")\n","for ex in tfds.as_numpy(nq_block(\"validation\").take(5)):\n","  print(ex)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"Hq0uLYNTjM9z","executionInfo":{"status":"ok","timestamp":1650237060076,"user_tz":-120,"elapsed":14,"user":{"displayName":"Cristian Buratti","userId":"16128402122774352339"}}},"outputs":[],"source":["def block_preprocessing(ds):\n","  \n","  def to_inputs_and_targets(ex):\n","\n","        inputs = tf.strings.join(['BLOCK:' + ex['input']], separator=' ')\n","        class_label = tf.strings.join([ex['output']], separator=' ')\n","        return {'inputs': inputs, 'targets': class_label }\n","    \n","  return ds.map(to_inputs_and_targets, \n","                num_parallel_calls=tf.data.experimental.AUTOTUNE)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1650237060077,"user":{"displayName":"Cristian Buratti","userId":"16128402122774352339"},"user_tz":-120},"id":"ji4u8yhqjNER","outputId":"973db719-3efe-4093-b7a0-9bf202be6101"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<t5.data.dataset_providers.FunctionTask at 0x7f59088b2a90>"]},"metadata":{},"execution_count":18}],"source":["t5.data.TaskRegistry.remove('block')\n","t5.data.TaskRegistry.add(\n","    \"block\",\n","    dataset_fn=nq_block,\n","    splits=[\"train\", \"validation\"],\n","    text_preprocessor=[block_preprocessing],\n","    output_features = DEFAULT_OUTPUT_FEATURES,\n","    metric_fns=[t5.evaluation.metrics.accuracy],\n","    num_input_examples=num_nq_examples_block\n",")"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2559,"status":"ok","timestamp":1650237062629,"user":{"displayName":"Cristian Buratti","userId":"16128402122774352339"},"user_tz":-120},"id":"JG09lDZdjNKr","outputId":"49f8ebbb-0e24-4237-da17-1b9fb0d07605"},"outputs":[{"output_type":"stream","name":"stdout","text":["A few preprocessed training examples...\n","{'inputs_pretokenized': b'BLOCK:public static void toastShortMessage(final Activity activity, final String message) { if (!TextUtils.isEmpty(message)) { activity.runOnUiThread(new Runnable() { @Override public void run() <extra_id_0> }); } } <SEP> /** * Displays a short toast message on the screen. * * @param activity The activity that short message is displayed on it. * @param message The message to be displayed. */', 'inputs': array([17090,    78,  2825,   190,    65,    16,  2077,  6022,   538,\n","         451,  8685, 10647,  2223,     9,    85,    53,   504,    91,\n","          20,    56,     4,   125,  1147,   821,    12,   660,  3579,\n","         451,  5619,  6275,    20,  2223,    12,  5842,  1033, 10859,\n","        3369,   451,  1973,  6854,  1927,    20,    55,  8203,    38,\n","          65,   506,  1927, 32099,    21,  4767,    21,    21,    52,\n","       25390,  1302,     6, 23808,   651, 10120,     8,    18,   725,\n","          16,  2077,   504,    44,    11,  1674,    12,   651,   651,\n","          55, 11404,  2223,    69,  2223,    43,   725,   504,    28,\n","        6401,    44,    62,    12,   651,    55, 11404,   504,    69,\n","         504,    16,    54,  6401,    12,   651,    98,     1],\n","      dtype=int32), 'targets_pretokenized': b'{ Toast.makeText(activity, message, Toast.LENGTH_LONG).show(); }', 'targets': array([   20, 24914,    12, 12819,  1147,   451, 15765,     9,   504,\n","           9, 24914,    12,  9421,    35, 14035,   437,  7420, 12329,\n","          21,     1], dtype=int32)}\n","{'inputs_pretokenized': b'BLOCK:protected void checkPermissionAndLaunchCamera() { int permissionCheck = ContextCompat.checkSelfPermission(this, Manifest.permission.CAMERA); if (permissionCheck == PackageManager.PERMISSION_GRANTED) { launchCamera(); } else { if (ActivityCompat.shouldShowRequestPermissionRationale(this, Manifest.permission.CAMERA)) { onRequiredCameraPermissionRationale(); } else <extra_id_0> } } <SEP> /** * This is the API method which will initiate the camera picture taking process. */', 'inputs': array([17090,    78, 24154,    65, 19855,   703, 10772, 24982,  1927,\n","          20,    99,  4157,  2722,    24,  3249, 11511,  1097,    12,\n","        5595, 10642,  3010,   451,  5885,     9, 22906,    12, 21971,\n","          12, 20712, 17133,  4767,    56,     4, 21971,  2722,   126,\n","        8707,   485,    12, 24641,    35, 24129,  1854,    91,    20,\n","        1856, 24982, 12329,    21,   203,    20,    56,     4,  5733,\n","       11511,  1097,    12,  7101,  7598,   334,  3010,   806,  1077,\n","          63,   451,  5885,     9, 22906,    12, 21971,    12, 20712,\n","       17133,  6275,    20,    44,  4689, 24982,  3010,   806,  1077,\n","          63, 12329,    21,   203, 32099,    21,    21,    52, 25390,\n","        1302,     6, 23808,   651,   214,    28,    11,  4679,   765,\n","         138,    74, 14342,    11,  2368,   940,  1774,    27,   416,\n","          12,   651,    98,     1], dtype=int32), 'targets_pretokenized': b'{ ActivityCompat.requestPermissions(this, new String[]{Manifest.permission.CAMERA}, REQUEST_CAMERA_PERMISSION); }', 'targets': array([   20, 10647, 11511,  1097,    12,  7867,  6169,   451,  5885,\n","           9,    39,    53, 11459,  4828, 12359,    12, 21971,    12,\n","       20712, 17133,  2202,     9, 12680,    35, 20712, 17133,    35,\n","       24641,  4767,    21,     1], dtype=int32)}\n","{'inputs_pretokenized': b'BLOCK:public void removeChildByTag(int tag, boolean cleanup) { assert tag != kCCNodeTagInvalid: \"Invalid tag\"; CCNode child = getChildByTag(tag); if (child == null) <extra_id_0> else removeChild(child, cleanup); } <SEP> /** Removes a child from the container by tag value. It will also cleanup all running actions depending on the cleanup parameter @since v0.7.1 */', 'inputs': array([17090,    78,  2825,    65,   613,  2897,   950,  1864,   451,\n","        2886,  2133,     9,   177,  9389,    91,    20,   593,  2133,\n","         161,   376,  4541,   421,  1864,  3248,    78,    29,  3248,\n","        2133,    26,   204,  8949,   421,   710,    24,  7423,   950,\n","        1864,   451,  5522,  4767,    56,     4,  8466,   126,    70,\n","          91, 32099,   203,   613,  2897,   451,  8466,     9,  9389,\n","        4767,    21,    52, 25390,  1302,     6, 23808,  9254,     8,\n","          18,   710,    67,    11,  1648,    76,  2133,   148,    12,\n","         165,    74,   137,  9389,    92,  1550,  4088,  4449,    44,\n","          11,  9389,  2469,    55, 23268,   358, 26177,    12,   111,\n","         651,    98,     1], dtype=int32), 'targets_pretokenized': b'Log.w(LOG_TAG, \"removeChild: child not found\");', 'targets': array([ 4763,    12,   130,   451,  9957,    35,  9245,     9,    29,\n","       12727,  2897,    78,   710,    86,   453,    26,  4767,     1],\n","      dtype=int32)}\n","{'inputs_pretokenized': b'BLOCK:public void playMediaObject(@NonNull final Playable playable, final boolean stream, final boolean startWhenPrepared, final boolean prepareImmediately) { Log.d(TAG, \"playMediaObject(...)\"); useCallerThread = UserPreferences.useExoplayer(); executor.submit(() -> { playerLock.lock(); try <extra_id_0> catch (RuntimeException e) { e.printStackTrace(); throw e; } finally { playerLock.unlock(); } }); } <SEP> /** * Starts or prepares playback of the specified Playable object. If another Playable object is already being played, the currently playing * episode will be stopped and replaced with the new Playable object. If the Playable object is already being played, the method will * not do anything. * Whether playback starts immediately depends on the given parameters. See below for more details. * <p/> * States: * During execution of the method, the object will be in the INITIALIZING state. The end state depends on the given parameters. * <p/> * If \\'prepareImmediately\\' is set to true, the method will go into PREPARING state and after that into PREPARED state. If * \\'startWhenPrepared\\' is set to true, the method will additionally go into PLAYING state. * <p/> * If an unexpected error occurs while loading the Playable\\'s metadata or while setting the MediaPlayers data source, the object * will enter the ERROR state. * <p/> * This method is executed on an internal executor service. * * @param playable The Playable object that is supposed to be played. This parameter must not be null. * @param stream The type of playback. If false, the Playable object MUST provide access to a locally available file via * getLocalMediaUrl. If true, the Playable object MUST provide access to a resource that can be streamed by * the Android MediaPlayer via getStreamUrl. * @param startWhenPrepared Sets the \\'startWhenPrepared\\' flag. This flag determines whether playback will start immediately after the * episode has been prepared for playback. Setting this flag to true does NOT mean that the episode will be prepared * for playback immediately (see \\'prepareImmediately\\' parameter for more details) * @param prepareImmediately Set to true if the method should also prepare the episode for playback. */', 'inputs': array([17090,    78,  2825,    65,   423,  9240,   587, 23610, 18588,\n","          85,  4029,   202,   423,   202,     9,    85,   177,   819,\n","           9,    85,   177,   305,  2230, 19853,     9,    85,   177,\n","        2041, 21285,    91,    20,  4763,    12,    34,   451,  9245,\n","           9,    29,  5314,  9240,   587, 20332,    26,  4767,   197,\n","       14364,  3369,    24,  1667,  8123,    12,  1560,  5757,   233,\n","       12015, 12329,  5509,    12, 21258,   451,  1927,   394,    20,\n","        2000,  2953,    12,  5818, 12329,   226, 32099,   328,     4,\n","        1894,    77,    91,    20,    77,    12,  7029, 19268, 12329,\n","         235,    77,   204,    21,   966,    20,  2000,  2953,    12,\n","        2331,  5818, 12329,    21,    21,  4767,    21,    52, 25390,\n","        1302,     6, 23808,   651,  5573,     8,    66,  2041,     8,\n","         423,  2257,    19,    11,  5731,  4029,   202,   921,    12,\n","         354,   713,  4029,   202,   921,    28,   945,   455,   423,\n","          47,     9,    11,  1426,   423,    27,   651,  4467,    74,\n","          54,  6235,    13,  6303,    41,    11,    39,  4029,   202,\n","         921,    12,   354,    11,  4029,   202,   921,    28,   945,\n","         455,   423,    47,     9,    11,   765,    74,   651,    86,\n","         123,  1714,    12,   651,     6,  2762,   423,  2257,  5172,\n","        3176,  7539,    44,    11,  1017,  1622,    12,  3380,   826,\n","          32,   112,  1193,    12,   651,    52,   374,    98,  1302,\n","         651,  2093,    78,   651,     6,  4384,  3668,    19,    11,\n","         765,     9,    11,   921,    74,    54,    22,    11, 18232,\n","       22110,  2388,   398,    12,    69,   365,   398,  7539,    44,\n","          11,  1017,  1622,    12,   651,    52,   374,    98,  1302,\n","         651,   354,     6,    42, 30537, 21285,    42,    28,    94,\n","          16,   173,     9,    11,   765,    74,   236,   216, 13636,\n","       23750,  2388,   398,    13,   272,    43,   216,     6, 30548,\n","         347,   398,    12,   354,   651,     6,    42,  3890,  2230,\n","       19853,    42,    28,    94,    16,   173,     9,    11,   765,\n","          74,  1313,    59,   236,   216,     6, 23709,  2388,   398,\n","          12,   651,    52,   374,    98,  1302,   651,   354,    81,\n","        6443,  1020,  3870,     8,   317, 10157,    11,  4029,   202,\n","          42,     8,  2233,    66,   317,  2360,    11,  4114,  6691,\n","           8,   261,   582,     9,    11,   921,   651,    74,  1363,\n","          11,  7146,   398,    12,   651,    52,   374,    98,  1302,\n","         651,   214,   765,    28, 11456,    44,    81,  2242,  5509,\n","         289,    12,   651,   651,    55, 11404,   423,   202,    69,\n","        4029,   202,   921,    43,    28,  6869,    16,    54,   423,\n","          47,    12,   214,  2469,   549,    86,    54,    70,    12,\n","         651,    55, 11404,   819,    69,   269,    19,   423,  2257,\n","          12,   354,   193,     9,    11,  4029,   202,   921, 15627,\n","         471,   737,    16,    18,  8306,   395,   419,  1605,   651,\n","        9895,  9240,  1366,    12,   354,   173,     9,    11,  4029,\n","         202,   921, 15627,   471,   737,    16,    18,   983,    43,\n","          72,    54,   819,    47,    76,   651,    11,  4009,  4114,\n","        6691,  1605, 15883,  1366,    12,   651,    55, 11404,   305,\n","        2230, 19853,  9883,    11,     6,    42,  3890,  2230, 19853,\n","          42,  4894,    12,   214,  4894,  3006,     8,  1563,   423,\n","        2257,    74,   305,  3176,   272,    11,   651,  4467,    87,\n","         168,  3382,    32,   423,  2257,    12, 15832,    49,  4894,\n","          16,   173,   642,  3327,   942,    43,    11,  4467,    74,\n","          54,  3382,   651,    32,   423,  2257,  3176,     4,  3435,\n","           6,    42, 30537, 21285,    42,  2469,    32,   112,  1193,\n","          91,   651,    55, 11404,  2041, 21285,   726,    16,   173,\n","          56,    11,   765,   270,   137,  2041,    11,  4467,    32,\n","         423,  2257,    12,   651,    98,     1], dtype=int32), 'targets_pretokenized': b'{ playMediaObject(playable, false, stream, startWhenPrepared, prepareImmediately); }', 'targets': array([   20,   423,  9240,   587,   451,  5314,   202,     9,   193,\n","           9,   819,     9,   305,  2230, 19853,     9,  2041, 21285,\n","        4767,    21,     1], dtype=int32)}\n","{'inputs_pretokenized': b'BLOCK:public static String getString(Object obj) { if (obj == null) { return \"\"; } if (obj instanceof byte[]) { try { return new String((byte[]) obj, CODING_CHARSET); } catch (UnsupportedEncodingException e) <extra_id_0> } else if (obj instanceof char[]) { return new String((char[]) obj); } else { return (String) obj; } } <SEP> /** * Convert <code>obj</code> to <code>String</code>. If obj is byte[], * then using UTF-8 charset. when <code>obj</code> is <code>null</code>, * empty String would be returned. * * @param obj * object to be covert * @return UTF-8 String */', 'inputs': array([17090,    78,  2825,   190,    53,  1996,   451,   587,   845,\n","          91,    20,    56,     4, 17497,   126,    70,    91,    20,\n","          40,  1036,   204,    21,    56,     4, 17497,   603,   576,\n","       11459,    91,    20,   226,    20,    40,    39,    53, 11802,\n","       11376, 11459,    91,   845,     9,  6098, 23131,    35, 24034,\n","        4767,    21,   328,     4, 10457,  5055,   117,    77,    91,\n","       32099,    21,   203,    56,     4, 17497,   603,  1450, 11459,\n","          91,    20,    40,    39,    53, 11802, 12471, 11459,    91,\n","         845,  4767,    21,   203,    20,    40,     4,   529,    91,\n","         845,   204,    21,    21,    52, 25390,  1302,     6, 23808,\n","         651, 15928,    52,  3927,  1302, 17497,  3603,  3927,  1302,\n","          16,    52,  3927,  1302,   529,  3603,  3927,  1302,    12,\n","         354,   845,    28,   576, 11459,     9,   651,   477,   461,\n","        5480,  3677,  9230,    12,   139,    52,  3927,  1302, 17497,\n","        3603,  3927,  1302,    28,    52,  3927,  1302,  4005,  3603,\n","        3927,  1302,     9,   651,  1806,    53,   220,    54,  5341,\n","          12,   651,   651,    55, 11404,   845,   651,   921,    16,\n","          54,  1364,    46,   651,    55,  8529,  5480,  3677,    53,\n","         651,    98,     1], dtype=int32), 'targets_pretokenized': b'{ return \"\"; }', 'targets': array([  20,   40, 1036,  204,   21,    1], dtype=int32)}\n"]}],"source":["nq_task = t5.data.TaskRegistry.get(\"block\")\n","ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": SEQ_LENGTH, \"targets\": SEQ_LENGTH})\n","print(\"A few preprocessed training examples...\")\n","for ex in tfds.as_numpy(ds.take(5)):\n","  print(ex)\n"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"H0chadICjOT_","executionInfo":{"status":"ok","timestamp":1650237062630,"user_tz":-120,"elapsed":8,"user":{"displayName":"Cristian Buratti","userId":"16128402122774352339"}}},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"usj4CkZEc_y_"},"source":["### Finetuning\n","You can run the finetuning using the following cells.  \n","Please set the correct path of the variable *MODEL_DIR* (the path to save the pretrained model in), *PATH_GIN_FILE* (the gin file configuration for this finetuning) and *PRETRAINED_DIR* (the folder that contains the pretrained model).  \n","**Keep attention** to change the *pretrained_model_dir* in finetune step (if you are starting the finetuning from scratch you have to set the value *PRETRAINED_DIR*, if you are restarting the finetuning from a previous saved checkpoint you have to set the value *MODEL_DIR*)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1650237062630,"user":{"displayName":"Cristian Buratti","userId":"16128402122774352339"},"user_tz":-120},"id":"cz1a1TxFNKmx","outputId":"caf9e7f0-609b-4382-9a5d-d95de73ecbe6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<seqio.dataset_providers.Mixture at 0x7f59084548d0>"]},"metadata":{},"execution_count":20}],"source":["def _rate_num_input_examples(task):\n","  if \"train\" in task.splits:\n","    return float(task.num_input_examples(\"train\"))\n","  elif \"validation\" in task.splits:\n","    return float(task.num_input_examples(\"validation\"))\n","  else:\n","    raise ValueError(\"Task %s does not have a train or validation split.\" % (task.name))\n","\n","\n","t5.data.MixtureRegistry.remove(\"all_tasks\")\n","t5.data.MixtureRegistry.add(\n","    \"all_tasks\",\n","    [\"construct\", \"token\", \"block\"],\n","    default_rate=_rate_num_input_examples\n","     #default_rate=1.0\n",")"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"_3Qx699vN302","executionInfo":{"status":"ok","timestamp":1650237069179,"user_tz":-120,"elapsed":6553,"user":{"displayName":"Cristian Buratti","userId":"16128402122774352339"}}},"outputs":[],"source":["from mesh_tensorflow.transformer.learning_rate_schedules import slanted_triangular\n","# C.B.: Added import\n","import t5.models\n","\n","MODEL_SIZE = \"small\" \n","\n","# Set the folder where the checkpoints and all the others information will be writed\n","MODEL_DIR = BASE_DIR + '/T5_extension/finetuning'\n","\n","# Specify the pre-trained dir which must contain the pre-trained models, the operative_config.gin file and the checkpoint file as well\n","PRETRAINED_DIR = BASE_DIR + '/T5_extension/pretrained_model'\n","\n","\n","model_parallelism, train_batch_size, keep_checkpoint_max = {\n","    \"small\": (1, 256, 16000),\n","    \"base\": (2, 128, 8),\n","    \"large\": (8, 64, 4),\n","    \"3B\": (8, 16, 1),\n","    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n","\n","tf.io.gfile.makedirs(MODEL_DIR)\n","\n","\n","# C.B.: Change to 100 for last phase, default 5000\n","SAVE_CHECKPOINTS_STEPS = 5000\n","\n","\n","model = t5.models.MtfModel(\n","    model_dir=MODEL_DIR,\n","    tpu=TPU_ADDRESS,\n","    tpu_topology=TPU_TOPOLOGY,\n","    model_parallelism=model_parallelism,\n","    batch_size=train_batch_size,\n","    learning_rate_schedule = slanted_triangular,\n","    sequence_length={\"inputs\": SEQ_LENGTH, \"targets\": SEQ_LENGTH},\n","    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n","    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n","    iterations_per_loop=100,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6oHp5ScE7nf2","outputId":"5df217e7-9717-4aed-8abb-dd4a6abf135b"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:system_path_file_exists:gs://code-generation/T5_extension/configuration_file/operative_config.gin\n","ERROR:root:Path not found: gs://code-generation/T5_extension/configuration_file/operative_config.gin\n","INFO:root:system_path_file_exists:gs://code-generation/T5_extension/finetuning/operative_config.gin\n","ERROR:root:Path not found: gs://code-generation/T5_extension/finetuning/operative_config.gin\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Using config: {'_model_dir': 'gs://code-generation/T5_extension/finetuning', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 5000, '_save_checkpoints_secs': None, '_session_config': graph_options {\n","  rewrite_options {\n","    disable_meta_optimizer: true\n","  }\n","}\n","cluster_def {\n","  job {\n","    name: \"worker\"\n","    tasks {\n","      key: 0\n","      value: \"10.24.154.106:8470\"\n","    }\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.24.154.106:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.24.154.106:8470', '_evaluation_master': 'grpc://10.24.154.106:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7f590866a090>}\n","INFO:tensorflow:_TPUContext: eval_on_tpu True\n","INFO:tensorflow:Querying Tensorflow master (grpc://10.24.154.106:8470) for TPU system metadata.\n","INFO:tensorflow:Initializing TPU system (master: grpc://10.24.154.106:8470) to fetch topology for model parallelism. This might take a while.\n","INFO:tensorflow:Found TPU system:\n","INFO:tensorflow:*** Num TPU Cores: 8\n","INFO:tensorflow:*** Num TPU Workers: 1\n","INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 8203554969501215935)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 1924040422112234054)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -4057290797005669941)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -7952488969553160909)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4979056859487874379)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, -2460552859870074430)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -7364848697556103116)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, -2652901520188678716)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4231896996839686263)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, -5949526336605652176)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6450791641836753683)\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","INFO:tensorflow:Calling model_fn.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/seqio/dataset_providers.py:1456: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.sample_from_datasets(...)`.\n","INFO:tensorflow:num_cores_per_replica: 1\n","INFO:tensorflow:computation_shape: [1, 1, 1, 1]\n","INFO:tensorflow:num_replicas: 8\n","INFO:tensorflow:device_assignment.topology.device_coordinates: [[[0 0 0 0]\n","  [0 0 0 1]\n","  [1 0 0 0]\n","  [1 0 0 1]\n","  [0 1 0 0]\n","  [0 1 0 1]\n","  [1 1 0 0]\n","  [1 1 0 1]]]\n","INFO:tensorflow:device_assignment.core_assignment: [[[0 0 0 0]]\n","\n"," [[0 0 0 1]]\n","\n"," [[1 0 0 0]]\n","\n"," [[1 0 0 1]]\n","\n"," [[0 1 0 0]]\n","\n"," [[0 1 0 1]]\n","\n"," [[1 1 0 0]]\n","\n"," [[1 1 0 1]]]\n","INFO:tensorflow:auto_logical_to_physical_tpu logical_shape=[8] physical_shape=[2, 2, 2]\n","INFO:tensorflow:auto_logical_to_physical_tpu logical_to_physical = [(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 1, 0), (1, 1, 1), (1, 0, 0), (1, 0, 1)]\n","WARNING:tensorflow:SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n","INFO:tensorflow:SimdMeshImpl init: Shape[batch=8] LayoutRules{('vocab', 'model'), ('experts', 'batch'), ('batch', 'batch'), ('ensemble', 'ensemble'), ('d_ff', 'model'), ('heads', 'model')}\n","INFO:tensorflow:Device Assignment: <tensorflow.python.tpu.device_assignment.DeviceAssignment object at 0x7f5904905a50>\n","WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n","WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n","INFO:tensorflow:Create pnum_tensor\n","INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_000/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable decoder/block_000/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_001/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable decoder/block_001/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_002/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable decoder/block_002/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_003/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable decoder/block_003/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_004/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable decoder/block_004/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_005/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable decoder/block_005/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_000/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable encoder/block_000/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_001/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable encoder/block_001/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_002/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable encoder/block_002/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_003/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable encoder/block_003/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_004/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable encoder/block_004/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_005/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable encoder/block_005/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable shared/embedding                                             size 16449536     slice_size 16449536     Shape[vocab=32128, d_model=512]                             \n","INFO:tensorflow:Variable stacked/encoder/block_000/layer_000/SelfAttention/relative_attention_bias size 512          slice_size 512          Shape[stacked=2, heads=8, buckets=32]                       \n","INFO:tensorflow:    encoder/block_000/layer_000/SelfAttention/relative_attention_bias\n","INFO:tensorflow:    decoder/block_000/layer_000/SelfAttention/relative_attention_bias\n","INFO:tensorflow:Variable stacked/encoder/block_000/layer_000/layer_norm/scale         size 16384        slice_size 16384        Shape[stacked=32, d_model=512]                              \n","INFO:tensorflow:    encoder/block_000/layer_000/layer_norm/scale\n","INFO:tensorflow:    encoder/block_000/layer_001/layer_norm/scale\n","INFO:tensorflow:    encoder/block_001/layer_000/layer_norm/scale\n","INFO:tensorflow:    encoder/block_001/layer_001/layer_norm/scale\n","INFO:tensorflow:    encoder/block_002/layer_000/layer_norm/scale\n","INFO:tensorflow:    encoder/block_002/layer_001/layer_norm/scale\n","INFO:tensorflow:    encoder/block_003/layer_000/layer_norm/scale\n","INFO:tensorflow:    encoder/block_003/layer_001/layer_norm/scale\n","INFO:tensorflow:    encoder/block_004/layer_000/layer_norm/scale\n","INFO:tensorflow:    encoder/block_004/layer_001/layer_norm/scale\n","INFO:tensorflow:    encoder/block_005/layer_000/layer_norm/scale\n","INFO:tensorflow:    encoder/block_005/layer_001/layer_norm/scale\n","INFO:tensorflow:    encoder/final_layer_norm/scale\n","INFO:tensorflow:    decoder/block_000/layer_000/layer_norm/scale\n","INFO:tensorflow:    decoder/block_000/layer_001/layer_norm/scale\n","INFO:tensorflow:    decoder/block_000/layer_002/layer_norm/scale\n","INFO:tensorflow:    decoder/block_001/layer_000/layer_norm/scale\n","INFO:tensorflow:    decoder/block_001/layer_001/layer_norm/scale\n","INFO:tensorflow:    decoder/block_001/layer_002/layer_norm/scale\n","INFO:tensorflow:    decoder/block_002/layer_000/layer_norm/scale\n","INFO:tensorflow:    decoder/block_002/layer_001/layer_norm/scale\n","INFO:tensorflow:    decoder/block_002/layer_002/layer_norm/scale\n","INFO:tensorflow:    decoder/block_003/layer_000/layer_norm/scale\n","INFO:tensorflow:    decoder/block_003/layer_001/layer_norm/scale\n","INFO:tensorflow:    decoder/block_003/layer_002/layer_norm/scale\n","INFO:tensorflow:    decoder/block_004/layer_000/layer_norm/scale\n","INFO:tensorflow:    decoder/block_004/layer_001/layer_norm/scale\n","INFO:tensorflow:    decoder/block_004/layer_002/layer_norm/scale\n","INFO:tensorflow:    decoder/block_005/layer_000/layer_norm/scale\n","INFO:tensorflow:    decoder/block_005/layer_001/layer_norm/scale\n","INFO:tensorflow:    decoder/block_005/layer_002/layer_norm/scale\n","INFO:tensorflow:    decoder/final_layer_norm/scale\n","INFO:tensorflow:Trainable Variables            count: 99      Total size: 60506624         Total slice_size: 60506624       \n","INFO:tensorflow:All Variables                  count: 105     Total size: 60691328         Total slice_size: 60691328       \n","INFO:tensorflow:Counters:\n","allreduce: 6.85e+08\n"," allreduce/[0]: 6.85e+08\n","  allreduce/[0]/einsum_op: 4.84e+08\n","  allreduce/[0]/reduce_op: 2.01e+08\n","einsum: 4.06e+13\n","einsum_unique: 4.05e+13\n","output: 3.66e+11\n"," output/AddOperation: 6.7e+10\n"," output/BinaryOpWithBroadcasting: 4.51e+09\n"," output/BroadcastOperation: 2.02e+10\n"," output/Constant: 8\n"," output/EinsumOperation: 1.46e+11\n"," output/ImportOperation: 6.29e+06\n"," output/MinMaxOperation: 3.79e+07\n"," output/OneHotOperation: 1.34e+10\n"," output/RandomOperation: 2.96e+07\n"," output/RangeOperation: 8.19e+03\n"," output/ReduceOperation: 7.93e+09\n"," output/ReshapeOperation: 1.38e+10\n"," output/ScalarAddOperation: 5.39e+08\n"," output/ScalarMultiplyOperation: 3.64e+09\n"," output/ShiftOperation: 2.62e+05\n"," output/SlicewiseOperation: 7.48e+10\n"," output/StackOperation: 1.35e+06\n"," output/StackedVariable: 1.35e+06\n"," output/StopGradient: 1.39e+10\n"," output/UnstackOperation: 1.35e+06\n"," output/Variable: 4.84e+08\n","output_unique: 3.58e+11\n"," output_unique/AddOperation: 6.67e+10\n"," output_unique/BinaryOpWithBroadcasting: 4.47e+09\n"," output_unique/BroadcastOperation: 2.02e+10\n"," output_unique/Constant: 1\n"," output_unique/EinsumOperation: 1.42e+11\n"," output_unique/ImportOperation: 7.87e+05\n"," output_unique/MinMaxOperation: 4.85e+06\n"," output_unique/OneHotOperation: 1.27e+10\n"," output_unique/RandomOperation: 2.96e+07\n"," output_unique/RangeOperation: 1.02e+03\n"," output_unique/ReduceOperation: 7.76e+09\n"," output_unique/ReshapeOperation: 1.38e+10\n"," output_unique/ScalarAddOperation: 7.1e+07\n"," output_unique/ScalarMultiplyOperation: 3.55e+09\n"," output_unique/ShiftOperation: 2.62e+05\n"," output_unique/SlicewiseOperation: 7.25e+10\n"," output_unique/StackOperation: 1.69e+05\n"," output_unique/StackedVariable: 1.69e+05\n"," output_unique/StopGradient: 1.39e+10\n"," output_unique/UnstackOperation: 1.69e+05\n"," output_unique/Variable: 6.05e+07\n","variables: 6.07e+07\n"," variables/trainable: 6.05e+07\n"," variables/untrainable: 1.85e+05\n","INFO:tensorflow:Initializing variables from gs://code-generation/T5_extension/finetuning/model.ckpt-895000:\n","INFO:tensorflow:Variables in gs://code-generation/T5_extension/finetuning/model.ckpt-895000 but not in graph:\n","INFO:tensorflow:\n","INFO:tensorflow:Variables in graph but not in gs://code-generation/T5_extension/finetuning/model.ckpt-895000:\n","INFO:tensorflow:\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Bypassing TPUEstimator hook\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:TPU job name worker\n","INFO:tensorflow:Starting the session.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from gs://code-generation/T5_extension/finetuning/model.ckpt-895000\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1161: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file utilities to get mtimes.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:758: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Prefer Variable.assign which has equivalent behavior in 2.X.\n","INFO:tensorflow:Initialized dataset iterators in 0 seconds\n","INFO:tensorflow:Installing graceful shutdown hook.\n","INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n","INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n","\n","INFO:tensorflow:Starting infeed thread controller.\n","INFO:tensorflow:Starting outfeed thread controller.\n","INFO:tensorflow:Before copy master to slices.\n","INFO:tensorflow:Done with copy master to slices.\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 895000...\n","INFO:tensorflow:Before Save.\n","INFO:tensorflow:About to write a checkpoint\n","INFO:tensorflow:Saving checkpoints for 895000 into gs://code-generation/T5_extension/finetuning/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 895000...\n","INFO:tensorflow:Done writing checkpoint.\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (0, 0)\n","INFO:tensorflow:Outfeed finished for iteration (0, 96)\n","INFO:tensorflow:loss = 0.00016975403, step = 895100\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (1, 87)\n","INFO:tensorflow:loss = 0.00012683868, step = 895200 (63.449 sec)\n","INFO:tensorflow:global_step/sec: 1.57608\n","INFO:tensorflow:examples/sec: 403.476\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (2, 82)\n","INFO:tensorflow:loss = 0.00018501282, step = 895300 (63.160 sec)\n","INFO:tensorflow:global_step/sec: 1.5833\n","INFO:tensorflow:examples/sec: 405.324\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (3, 74)\n","INFO:tensorflow:loss = 0.00025177002, step = 895400 (65.539 sec)\n","INFO:tensorflow:global_step/sec: 1.52581\n","INFO:tensorflow:examples/sec: 390.606\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (4, 69)\n","INFO:tensorflow:loss = 0.00026893616, step = 895500 (63.159 sec)\n","INFO:tensorflow:global_step/sec: 1.5833\n","INFO:tensorflow:examples/sec: 405.325\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (5, 61)\n","INFO:tensorflow:loss = 0.00019454956, step = 895600 (65.510 sec)\n","INFO:tensorflow:global_step/sec: 1.52647\n","INFO:tensorflow:examples/sec: 390.777\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (6, 56)\n","INFO:tensorflow:loss = 0.00023365021, step = 895700 (65.502 sec)\n","INFO:tensorflow:global_step/sec: 1.52667\n","INFO:tensorflow:examples/sec: 390.827\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (7, 48)\n","INFO:tensorflow:loss = 0.0002155304, step = 895800 (63.160 sec)\n","INFO:tensorflow:global_step/sec: 1.58329\n","INFO:tensorflow:examples/sec: 405.323\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (8, 43)\n","INFO:tensorflow:loss = 0.0002040863, step = 895900 (63.160 sec)\n","INFO:tensorflow:global_step/sec: 1.58327\n","INFO:tensorflow:examples/sec: 405.318\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (9, 35)\n","INFO:tensorflow:loss = 0.00021648407, step = 896000 (65.521 sec)\n","INFO:tensorflow:global_step/sec: 1.52623\n","INFO:tensorflow:examples/sec: 390.716\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (10, 30)\n","INFO:tensorflow:loss = 0.00020217896, step = 896100 (63.160 sec)\n","INFO:tensorflow:global_step/sec: 1.58329\n","INFO:tensorflow:examples/sec: 405.323\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (11, 22)\n","INFO:tensorflow:loss = 0.00021457672, step = 896200 (65.522 sec)\n","INFO:tensorflow:global_step/sec: 1.52621\n","INFO:tensorflow:examples/sec: 390.71\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (12, 17)\n","INFO:tensorflow:loss = 0.00019550323, step = 896300 (65.548 sec)\n","INFO:tensorflow:global_step/sec: 1.5256\n","INFO:tensorflow:examples/sec: 390.553\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (13, 9)\n","INFO:tensorflow:loss = 0.00015640259, step = 896400 (63.159 sec)\n","INFO:tensorflow:global_step/sec: 1.5833\n","INFO:tensorflow:examples/sec: 405.325\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (14, 4)\n","INFO:tensorflow:loss = 0.00019550323, step = 896500 (63.159 sec)\n","INFO:tensorflow:global_step/sec: 1.58331\n","INFO:tensorflow:examples/sec: 405.326\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (15, 0)\n","INFO:tensorflow:Outfeed finished for iteration (15, 96)\n","INFO:tensorflow:loss = 0.00019550323, step = 896600 (65.506 sec)\n","INFO:tensorflow:global_step/sec: 1.52658\n","INFO:tensorflow:examples/sec: 390.806\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (16, 91)\n","INFO:tensorflow:loss = 0.00021743774, step = 896700 (63.159 sec)\n","INFO:tensorflow:global_step/sec: 1.58331\n","INFO:tensorflow:examples/sec: 405.327\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (17, 83)\n","INFO:tensorflow:loss = 0.00017166138, step = 896800 (65.507 sec)\n","INFO:tensorflow:global_step/sec: 1.52656\n","INFO:tensorflow:examples/sec: 390.798\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (18, 78)\n","INFO:tensorflow:loss = 0.00018692017, step = 896900 (65.551 sec)\n","INFO:tensorflow:global_step/sec: 1.52552\n","INFO:tensorflow:examples/sec: 390.534\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (19, 70)\n","INFO:tensorflow:loss = 0.00019073486, step = 897000 (63.160 sec)\n","INFO:tensorflow:global_step/sec: 1.58329\n","INFO:tensorflow:examples/sec: 405.322\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (20, 65)\n","INFO:tensorflow:loss = 0.00026130676, step = 897100 (63.160 sec)\n","INFO:tensorflow:global_step/sec: 1.58329\n","INFO:tensorflow:examples/sec: 405.322\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (21, 57)\n","INFO:tensorflow:loss = 0.00016784668, step = 897200 (65.519 sec)\n","INFO:tensorflow:global_step/sec: 1.52627\n","INFO:tensorflow:examples/sec: 390.726\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (22, 52)\n","INFO:tensorflow:loss = 0.00019741058, step = 897300 (63.159 sec)\n","INFO:tensorflow:global_step/sec: 1.5833\n","INFO:tensorflow:examples/sec: 405.326\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (23, 44)\n","INFO:tensorflow:loss = 0.00018596649, step = 897400 (65.536 sec)\n","INFO:tensorflow:global_step/sec: 1.52587\n","INFO:tensorflow:examples/sec: 390.622\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (24, 39)\n","INFO:tensorflow:loss = 0.00024604797, step = 897500 (65.475 sec)\n","INFO:tensorflow:global_step/sec: 1.52729\n","INFO:tensorflow:examples/sec: 390.985\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (25, 31)\n","INFO:tensorflow:loss = 0.00016498566, step = 897600 (63.161 sec)\n","INFO:tensorflow:global_step/sec: 1.58327\n","INFO:tensorflow:examples/sec: 405.318\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (26, 26)\n","INFO:tensorflow:loss = 0.00018596649, step = 897700 (63.159 sec)\n","INFO:tensorflow:global_step/sec: 1.58331\n","INFO:tensorflow:examples/sec: 405.328\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (27, 18)\n","INFO:tensorflow:loss = 0.00019741058, step = 897800 (65.454 sec)\n","INFO:tensorflow:global_step/sec: 1.52779\n","INFO:tensorflow:examples/sec: 391.113\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (28, 13)\n","INFO:tensorflow:loss = 0.00018787384, step = 897900 (63.158 sec)\n","INFO:tensorflow:global_step/sec: 1.58333\n","INFO:tensorflow:examples/sec: 405.331\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (29, 5)\n","INFO:tensorflow:loss = 0.00020503998, step = 898000 (65.546 sec)\n","INFO:tensorflow:global_step/sec: 1.52565\n","INFO:tensorflow:examples/sec: 390.566\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (30, 0)\n","INFO:tensorflow:Outfeed finished for iteration (30, 96)\n","INFO:tensorflow:loss = 0.00017547607, step = 898100 (65.540 sec)\n","INFO:tensorflow:global_step/sec: 1.52579\n","INFO:tensorflow:examples/sec: 390.602\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (31, 88)\n","INFO:tensorflow:loss = 0.00022125244, step = 898200 (63.157 sec)\n","INFO:tensorflow:global_step/sec: 1.58336\n","INFO:tensorflow:examples/sec: 405.339\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (32, 83)\n","INFO:tensorflow:loss = 0.0002784729, step = 898300 (63.159 sec)\n","INFO:tensorflow:global_step/sec: 1.5833\n","INFO:tensorflow:examples/sec: 405.325\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (33, 75)\n","INFO:tensorflow:loss = 0.00019073486, step = 898400 (65.493 sec)\n","INFO:tensorflow:global_step/sec: 1.52687\n","INFO:tensorflow:examples/sec: 390.879\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (34, 70)\n","INFO:tensorflow:loss = 0.00016117096, step = 898500 (63.160 sec)\n","INFO:tensorflow:global_step/sec: 1.58329\n","INFO:tensorflow:examples/sec: 405.322\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (35, 62)\n","INFO:tensorflow:loss = 0.00018501282, step = 898600 (65.545 sec)\n","INFO:tensorflow:global_step/sec: 1.52567\n","INFO:tensorflow:examples/sec: 390.572\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (36, 57)\n","INFO:tensorflow:loss = 0.00022602081, step = 898700 (65.482 sec)\n","INFO:tensorflow:global_step/sec: 1.52713\n","INFO:tensorflow:examples/sec: 390.944\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (37, 49)\n","INFO:tensorflow:loss = 0.00018882751, step = 898800 (63.162 sec)\n","INFO:tensorflow:global_step/sec: 1.58324\n","INFO:tensorflow:examples/sec: 405.31\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (38, 44)\n","INFO:tensorflow:loss = 0.00016498566, step = 898900 (63.159 sec)\n","INFO:tensorflow:global_step/sec: 1.5833\n","INFO:tensorflow:examples/sec: 405.325\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (39, 36)\n","INFO:tensorflow:loss = 0.00016784668, step = 899000 (65.472 sec)\n","INFO:tensorflow:global_step/sec: 1.52738\n","INFO:tensorflow:examples/sec: 391.009\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (40, 31)\n","INFO:tensorflow:loss = 0.00020885468, step = 899100 (63.159 sec)\n","INFO:tensorflow:global_step/sec: 1.5833\n","INFO:tensorflow:examples/sec: 405.326\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (41, 23)\n","INFO:tensorflow:loss = 0.00024795532, step = 899200 (65.502 sec)\n","INFO:tensorflow:global_step/sec: 1.52667\n","INFO:tensorflow:examples/sec: 390.828\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (42, 18)\n","INFO:tensorflow:loss = 0.00020980835, step = 899300 (65.444 sec)\n","INFO:tensorflow:global_step/sec: 1.52801\n","INFO:tensorflow:examples/sec: 391.172\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (43, 10)\n","INFO:tensorflow:loss = 0.00024032593, step = 899400 (63.160 sec)\n","INFO:tensorflow:global_step/sec: 1.58327\n","INFO:tensorflow:examples/sec: 405.318\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (44, 5)\n","INFO:tensorflow:loss = 0.00015640259, step = 899500 (63.159 sec)\n","INFO:tensorflow:global_step/sec: 1.58332\n","INFO:tensorflow:examples/sec: 405.329\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (45, 0)\n","INFO:tensorflow:Outfeed finished for iteration (45, 96)\n","INFO:tensorflow:loss = 0.00020980835, step = 899600 (65.520 sec)\n","INFO:tensorflow:global_step/sec: 1.52626\n","INFO:tensorflow:examples/sec: 390.724\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (46, 91)\n","INFO:tensorflow:loss = 0.0001821518, step = 899700 (63.159 sec)\n","INFO:tensorflow:global_step/sec: 1.58329\n","INFO:tensorflow:examples/sec: 405.323\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (47, 83)\n","INFO:tensorflow:loss = 0.00024223328, step = 899800 (65.559 sec)\n","INFO:tensorflow:global_step/sec: 1.52536\n","INFO:tensorflow:examples/sec: 390.492\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (48, 78)\n","INFO:tensorflow:loss = 0.00022125244, step = 899900 (65.472 sec)\n","INFO:tensorflow:global_step/sec: 1.52737\n","INFO:tensorflow:examples/sec: 391.006\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (49, 70)\n","INFO:tensorflow:loss = 0.00018882751, step = 900000 (63.158 sec)\n","INFO:tensorflow:global_step/sec: 1.58333\n","INFO:tensorflow:examples/sec: 405.332\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 900000...\n","INFO:tensorflow:Before Save.\n","INFO:tensorflow:About to write a checkpoint\n","INFO:tensorflow:Saving checkpoints for 900000 into gs://code-generation/T5_extension/finetuning/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 900000...\n","INFO:tensorflow:Done writing checkpoint.\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (50, 12)\n","INFO:tensorflow:loss = 0.00019550323, step = 900100 (96.665 sec)\n","INFO:tensorflow:global_step/sec: 1.0345\n","INFO:tensorflow:examples/sec: 264.832\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (51, 2)\n","INFO:tensorflow:Outfeed finished for iteration (51, 98)\n","INFO:tensorflow:loss = 0.00020980835, step = 900200 (66.499 sec)\n","INFO:tensorflow:global_step/sec: 1.50377\n","INFO:tensorflow:examples/sec: 384.966\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n"]}],"source":["PATH_GIN_FILE = BASE_DIR + '/T5_extension/configuration_file/operative_config.gin'\n","import gin\n","\n","with gin.unlock_config():\n","    gin.parse_config_file(PATH_GIN_FILE)\n","    #RUN FINE-TUNING\n","    FINETUNE_STEPS = 400000\n","    model.finetune(\n","        mixture_or_task_name=\"all_tasks\",\n","        # pretrained_model_dir=PRETRAINED_DIR,\n","        pretrained_model_dir=MODEL_DIR,\n","        finetune_steps=FINETUNE_STEPS\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bBXTPr0ke04I"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Fine_tuning.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}